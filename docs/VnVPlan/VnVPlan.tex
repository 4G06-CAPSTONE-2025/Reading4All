\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{bm}
\usepackage{float}

\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=black,
  linkcolor=red,
  urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
  \midrule
  Date 1 & 1.0 & Notes\\
  Date 2 & 1.1 & Notes\\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
  However, this does not mean listing every verification and
  validation technique
  that has ever been devised.  The VnV plan should also be a \textbf{feasible}
  plan. Execution of the plan should be possible with the time and
  team available.
  If the full plan cannot be completed during the time available, it
  can either be
  modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
  the design stage.  This means that the sections related to unit testing cannot
  initially be completed.  The sections will be filled in after the design stage
  is complete.  the final version of the VnV plan should have all
  sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}
\section*{Symbolic Constants}
\begin{longtable}{|p{8.0cm}|p{8.0cm}|}
\toprule
{\textbf{Name}} & {\textbf{Value}}\\
\midrule
T\_ALT\_GEN\_SMALL & 3 seconds(s) \\
T\_ALT\_GEN\_LARGE & 8 s \\
T\_UI\_RESP & 300 miliseconds (ms) \\
R\_SUFFICIENCY & 85\% \\
R\_LENGTH & 90\% \\
R\_USABILITY\_MEDIAN & 3 (rating) \\
R\_USABILITY\_MIN & 2 (rating) \\
T\_ERROR\_HANDLE & 2 s \\
T\_RECOVERY & 5 s \\
CAP\_CONCURRENT & 2 requests \\
CAP\_STORAGE & 500 images/day \\
MAINT\_TIME & 2 person-days/quarter \\
COMPAT\_VERSIONS & 2 releases \\
IMG\_SIZE\_BIG & 10 MEGABYTES (MB) \\
IMG\_SIZE\_SMALL & 2 MEGABYTES \\
TLS\_VERSION & 1.2 \\ 
FILE\_DELETE\_TIME & 60 s \\
FILE\_TYPES & .png, .jpg, .jpeg, .svg, .webp \\
NETWORK\_SOURCE\_POLICY & McMaster SSO tokens or IP ranges only\\
TEAM\_SIZE & 5 students \\
HOURS\_RESEARCH & 40 hours \\
HOURS\_BACKEND & 120 hours \\
HOURS\_FRONTEND & 80 hours \\
HOURS\_TESTING & 60 hours \\
HOURS\_DOCS & 30 hours \\
HOURS\_TOTAL & 330 hours \\
HOURS\_PROJECT & 1,320 person-hours \\
COST\_PER\_HOUR & \$20/hour \\
COST\_TOTAL & \$26,400 CAD \\
COST\_ACTUAL & \$0 CAD \\
COST\_INCENTIVE\_MIN & \$100 CAD \\
COST\_INCENTIVE\_MAX & \$150 CAD \\
MAX\_ZOOM\_PERCENTAGE & 200\% \qquad\textit{Table continues on next page}\\
MIN\_CONTRAST\_RATIO & 4.5:1\\
MAX\_UPLOAD\_STEPS & 5 steps\\
MAX\_MINUTES & 5 minutes\\
USERS\_SUCCESS\_PERCENT & 80\%\\
MAX\_ERROR\_RECOVER & 2 seconds \\
LEARNING\_PERCENT & 90\% \\
MAX\_LEARNING\_MINUTES & 5 minutes \\
MIN\_COMPENSATION\_DOLLARS & \$100 CAD \\ 
MAX\_COMPENSATION\_DOLLARS & \$150 CAD \\ 
LATEST\_RELEASES\_NUM & 3 \\
MOST\_COMMON\_SR & 3 \\
SFWR\_RELEASES & 2 \\


\bottomrule
\end{longtable}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
its general functions.}

The software being tested is called Reading4All. This software will utilize artificial intelligence (AI)/
machine learning (ML) techniques to provide detailed, context-informed alternative text for complex technical images, specifically those found in post-secondary Science, Technology, Engineering and Mathematics (STEM)
course materials. Reading4All will allow users to upload images and automatically produce corresponding alternative text (alt text), that meets the described criteria in Appendix~\ref{appendix:evaluation-metrics} . The system is intended for use by McMaster University students and faculty, therefore it will include user validation through McMaster's sign-on, ensuring that only verified users can access the Reading4All system.  
In addition, the system will allow users to edit the generated alternative text, view a history of uploaded images and their alternative text within that session and download the final outputs in their desired formats.


\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
just those that are most important.}


\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of
  limitations in your resources for verification and validation.  You
  can't do everything,
  so what are you going to prioritize?  As an example, if your system
  depends on an
  external library, you can explicitly state that you will assume
  that external library
has already been verified by its implementation team.}


The objective of this Verification and Validation (VnV) plan is to build confidence in the correctness, accessibility and usability of the Reading4All system. 
The plan focuses on ensuring that the system generates, accurate, detailed and contextually appropriate alternative text for complex STEM images, as this is the main functionality of the software. 
It also aims to verify that the systems interface is accessible an usable for individuals with disabilities, who are one of the main users of the software. The last objective is to demonstrate effective and accessible usability of secondary features
such as editing the outputted alternative text, viewing session history and file downloading. 
Verifying and validating these objectives is essential to ensure that users can benefit from the Reading4All system and 
it can effectively fulfill its goal of making STEM diagrams more accessible.
\\

Some objectives are out of scope from this VnV plan due to the time and resource limitations of the project.
The external libraries that might be used in the development of the system, including PyTorch, TensorFlow, Scikit-Learn, Pandas and frontend frameworks, will not be verified by our team, as they will be assumed to have been tested and validated by their implementation teams. 
The McMaster sign-on authentication service will also not be tested, as its maintained and run by the university.

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
  Your challenge level should exactly match what is included in your problem
  statement.  This should be the challenge level agreed on between you and the
  course instructor.  You can use a pull request to update your challenge level
  (in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
  can include usability testing, code walkthroughs, user documentation, formal
  proof, GenderMag personas, Design Thinking, etc.  Extras should have already
  been approved by the course instructor as included in your problem statement.
  You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}
\\
The challenge level of the project is set at a \textit{general} level that will include two additional 
components (extras). The first additional component will be a Norman's Principles report that will 
evaluate the design of our final product to ensure that it optimizes usability and accessibility. The second additional 
component will be a user manual that will include comprehensive documentation to guide users in using the final 
product effectively. 

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
documents and within those entries include a hyperlink to the documents.}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.} 
\\
The System Verification and Validation (VnV) plan will reference the following documents to aid in 
the project's assessment and testing:
\begin{enumerate}
  \item \textbf{Software Requirements Specification} (\citet{SRS}): This document outlines the key components 
for the VnV plan as it details the functional and non-functional requirements of the product. Ensuring that our testing 
satisfies these requirements is essential to meeting the goals of the project.
  \item \textbf{Design Document - Module Guide} (\citet{MG}): This document outlines how the system is divided into separate module and their respective functions. 
  This structure helps the VnV Plan by making it easier to test, trace, and confirm that each module works correctly and meets the requirements.
  \item \textbf{Design Document - Module Interface Specification} (\citet{MIS}): This document outlines how the modules of the system works and how they interact with each other.
  This aids in our VnV plan as it defines how to validate the testing of interactions between the individual parts of the system.
\end{enumerate}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize
this information.}

The V\*V team ensures that all SRS requirements are thorough, accurately implemented and tested to ensure our system meets them. 
Table ~\ref{tab:data-dictionary-reading4all} defines each Reading4All team members role in confirming that these requirements are well defined and fulfilled in the system.

\begin{table}[H]
    \centering
    \caption{Verification and Validation Responsibility Breakdown}
    \label{tab:data-dictionary-reading4all}
    \begin{tabular}{ |p{3.0cm}|p{3.8cm}|p{7.3cm}| }
      \hline
      \textbf{Name} & \textbf{ Focus Area } & \textbf{Responsibility} \\
      \hline
      Moly Mikhail and Fiza Sehar & Functional Requirements Tester & Completes manual testing on the software to ensure that functional requirements are met. Also oversees any written unit tests to ensure they correctly test the system and intended functionality \\
      \hline
      Nawaal Fatima & User Testing Preparation & Coordinates the user testing sessions, and oversees the planning and execution of the sessions. \\
      \hline 
      Casey Francine Bulaclac & Usability Requirements Tester & Completes manual testing to ensure all the defined usability requirements are met. Also references WCAG 2.1 crtieria is met, prior to automated testing. \\
      \hline 
      Dhruv Sardana & Performance, Operational and Environmental Requirements & Verifies that performance, operational and environmental requirements are thorough, clearly defined, attainable and measurable. Also, validates that the implemented system meets these requirements through extensive testing.  \\
      \hline 
      All Reading4All Team Members & Look and Feel Requirements, Usability and Humanity Requirements & All team members are responsible for verifying that the corresponding requrieemnts in the SRS are thorough, clearly defined, attainable and measurable. The team will validate that the final system meets the specified requirements prior to doing user testing. \\
      \hline
      Ms. Jingchuan Sui (Supervisor) & Overall Usability and Quality of Alt Text Reviewer, User Testing Material Reviewer& Reviews and provides feedback to team on the overall usability of the system and the quality of the generated alternative text. Reviews user testing material to ensure they align with accessibility standards and assits the team completing automating evaluation of the system against WCAG 2.1 criteria. \\ 
      \hline
    \end{tabular}
  \end{table}
  
  
\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
  read over the SRS.  You should explain your structured approach to the review.
  Will you have a meeting?  What will you present?  What questions will you ask?
  Will you give them instructions for a task-based inspection?  Will
  you use your
issue tracker?}

The verification of the Reading4All SRS will be completed
 through a team review, peer review, and supervisor feedback, in that order. 
 Feedback from each step will be incorporated iteratively to ensure the document is reviewed and the version presented to our supervisor reflects the most accurate and complete requirements.
 Furthermore, criteria not met from the checklists will be documented as GitHub issues, to ensure they are tracked and resolved. 
  \\

\textbf{Team Review}:
As the Reading4All team has already reviewed the outlined requirements in the SRS document, a team review will 
consist of each team member independently evaluating the document against the checklist provided in Appendix~\ref{appendix:srs_checklist}.   \\

\textbf{Peer Review}:
A peer review will be completed by Team 10 (One of a Kind) to provide feedback on our SRS document.
They will review the document against the same checklist provided in Appendix~\ref{appendix:srs_checklist}, ensuring consistency between how both teams evaluate the document. 
This process will allow external reviewers who understand the documents technical details and software engineering context to provide feedback on the completeness, understandability and the overall quality of the requirements. \\

\textbf{Supervisor Feedback}:
We will dedicate one of our weekly meetings with our supervisor to verifying the SRS document. 
During this meeting, will explain each functional and non functional requirement to ensure every requirement has been documented.
This process will help our supervisor gain an understanding of the specific requirements we have outlined and keep them in 
mind during future validation of our system. \\

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}
\\
The verification of our design will rely on reviews conducted by our fellow classmates and supervisor who will
evaluate our design documents including our Module Guide (MG) and Module Interface Specification (MIS). They will also aid in performing
consistency and traceability checks to ensure clear alignment between our requirements, design, and implementation. \\[1ex]
In a formal review, our supervisor, Ms. Jing, will verify that the user interface design supports accessibility in accordance with WCAG 2.1 standards. 
Additionally, our professor, Dr. Smith, will assess the technical aspects of the system, focusing on the machine learning architecture to ensure it is well designed.

\subsection{Verification and Validation Plan Verification}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}
The Verification and Validation Plan Verification for Reading4All will be completed through a team and peer review, as well as mutation testing to assess the effectiveness and correctness of our tests. 
Furthermore, any criteria not met, or failed mutation tests identified from this process will be documented as GitHub issues, to ensure they are tracked and resolved. 
\\

\textbf{Team and Peer Review:}
The Reading4All team will complete an internal review to ensure that the plan is complete and aligns with the SRS requirements, as well as other documents. 
Both our team and peer review will be completed by reviewing the V\&V document against the checklist provided in Appendix~\ref{appendix:v_v_checklist}.   \\ \\

\textbf{Mutation Testing:}
Mutation testing will be used to determine whether our unit tests correctly identify errors within the system.
Small errors will be intentionally introduced into the prototype code related to the main functionalities to check whether the test cases can detect unexpected behavior. 


\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
  walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

The implementation verification for Reading4All will utilize the unit tests defined in section~\ref{Unit Test Desc}, system tests defined in section~\ref{System Tests Desc},
static analyzers and code inspections.

\textbf{Static Analyzers}:
Static analyzers will be used to automatically review any committed code for errors, and ensure the implementation meets the specified coding standard.
Tools such as Flake8 and Coverage.py will be integrated into our development process. Flake8 will identify syntax and style issues, while Coverage.py can verify that any new code written has unit tests associated. 
Additionally, all unit tests will be executed automatically through GitHub Actions whenever new code is pushed or a pull request is opened. This will ensure that recent changes do not introduce any errors into previously completed features. 
Using these static analyzers and workflows will help us to continuously verify our implementation, ensuring it meets the design requirements, functional and non functional requirements.

\textbf{Code Inspections}:
As part of our development process, when a feature is completed, team members are required to open a pull request. The pull request highlights all the code changes made and is reviewed by other team members, before merging. 
This process ensures that the multiple team members are aware of the changes being made, and provides an opportunity to give feedback on 
the implementation decisions. Additionally it also helps verify that the code fulfills the intended functionality.
\subsection{Automated Testing and Verification Tools}
\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
Python.}



\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}
\\
The AI Generated Alternative (alt) Text tool will make use of the following automated testing and verification tools: 
\begin{itemize}
  \item \textbf{Automated Accessibility Testing}: Automated web accessibility testing tools such as the WAVE Web Accessibility Evaluation Tool will be used to 
  verify that the tool adheres to the WCAG 2.1 guidelines. These tools will automatically scan for accessibility issues such as missing alternative text and color contrast violations.
  \item \textbf{Unit Testing Framework, Linters, Coverage Tools, and Continuous Integration}: These tools have been detailed and outlined in the Expected Technologies section
  of our Development Plan (\citet{DP}) document under Table 3. 
\end{itemize}

\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
here.}

\wss{You might want to use review sessions with the stakeholder to check that
  the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should
  be used as an opportunity to validate the requirements.  You should plan on
  demonstrating your project to your supervisor shortly after the
  scheduled Rev 0 demo.
  The feedback from your supervisor will be very useful for improving
your project.}

\wss{For teams without an external supervisor, user testing can serve
  the same purpose
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}
\\
The AI Generated Alternative (alt) Text tool will be validated through the following: 
\begin{itemize}
  \item \textbf{User Testing}: Stakeholders of this project including students with disabilities
  will perform realistic tasks such as uploading an image and generating alt text to validate 
  that it meets their needs and requirements. The team will be present during these sessions to observe and note any challenges 
  the users face. The participants will also be asked follow-up questions highlighted in Appendix ~\ref{appendix:usability} to gain additional feedback and insight
  on their overall experience of using the tool. 
  \item \textbf{Formal Reviews}: Reviews will be conducted with the stakeholders of the project and our supervisor, Ms. Jing, to validate that the requirements
  outlined in our SRS document are correct and captures the needs for the tool. Additionally, the Rev 0 demo with Ms. Jing will serve as a validation review and allow the team 
  to receive feedback to improve the tool. 
\end{itemize} 

\section{System Tests}
\label{System Tests Desc}



This section details tests to cover all requirements as listed in the
Software Requirements Specification (SRS) for Team 22's project. The
tests ensure that the system performs to the predefined standards and
meets user needs.

\subsection{Tests for Functional Requirements}

The tests below cover all six functional requirements as defined in
the SRS document.(I need to add in more summary text here)

\paragraph{Tests for Functional Requirements}

\begin{enumerate}[label=FR-ST \arabic*., wide=0pt, leftmargin=*]
  \item{}
    {\bf Control:} Automatic. \\
    {\bf Initial State:} System is running and ready to accept an
    image upload.\\
    {\bf Input:} Upload files in the following formats:
    \begin{itemize}
      \item Valid: diagram1.jpeg, diagram2.png
      \item Invalid: diagram3.gif, diagram4.pdf
    \end{itemize}
    {\bf Output:} The system accepts .jpeg and .png images and
    displays an error message (e.g., “Invalid file type”) for .gif
    and .pdf uploads.\\
    {\bf  Case Derivation:}According to the \textbf{FR 1} criterion, the system
    must accept JPEG and PNG formats and reject all others with proper
    feedback. Therefore, valid formats are processed, and invalid
    formats trigger an error.\\
    {\bf How test will be performed:} The test can be performed by
    uploading a sample set of image files in different formats (JPEG,
    PNG, GIF, PDF) to the system. The system’s responses will be
    observed to confirm that only JPEG and PNG files are accepted,
    while others trigger an appropriate error message.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:} System running with image upload
    functionality active.\\
    {\bf Input:} Upload a set of test diagrams (diagram1.png, diagram2.jpeg).\\
    {\bf Output:} The system generates alternative text descriptions
    for each uploaded image that meet pre-determined quality or
    clarity criteria (e.g., contains key diagram elements, concise
    description, no missing components).\\
    {\bf  Case Derivation:}As specified by \textbf{FR 2} criterion,
    the system must correctly process JPEG and PNG files while
    rejecting all other formats. Therefore, the expected outcome is
    that valid images are accepted without error, and invalid formats
    trigger a clear feedback message to the user.\\
    {\bf How test will be performed:} The test can be performed by
    uploading a test set of sample diagrams to the system and
    reviewing the generated alternative text. The generated text will
    be compared against predetermined quality criteria or expected
    reference outputs to verify accuracy and completeness.\\

  \item{}
    {\bf Control:} Manual\\
    {\bf Initial State:} Alternative text has been generated for at
    least one uploaded image.\\
    {\bf Input:} Use screen readers such as NVDA, JAWS, and VoiceOver
    to read the outputted alternative text.\\
    {\bf Output:} Alternative text is fully read aloud by at least
    the most common screen readers without truncation, misreading, or
    formatting errors. \\
    {\bf  Case Derivation:} Given the \textbf{FR 3} criterion for
    compatibility with commonly used screen readers, the expected
    outcome is that the generated alternative text will be fully
    readable and correctly interpreted by tools such as NVDA, JAWS,
    and VoiceOver without truncation or mispronunciation.\\
    {\bf How test will be performed:} The test can be performed by
    enabling common screen readers such as NVDA, JAWS, and VoiceOver
    to read the generated alternative text aloud. Observations will
    confirm whether the text is read fully, clearly, and without
    formatting or accessibility issues.\\

  \item{}
    {\bf Control:} Manual\\
    {\bf Initial State:} Generated alternative text is visible to the user.\\
    {\bf Input:}User edits the outputted text (adds words, deletes
    sentences, modifies phrasing) and saves changes. \\
    {\bf Output:} The system reflects the user’s edits accurately and
    stores the updated version without loss of data or formatting errors.\\
    {\bf  Case Derivation:} As stated in \textbf{FR 4} criterion,
    users must be able to modify any part of the generated
    alternative text and save their changes. The expected result is
    that all edits are accurately captured, stored, and displayed
    without data loss or formatting issues.\\
    {\bf How test will be performed:} The test can be performed by
    selecting the generated alternative text and performing a series
    of edits—adding, deleting, and modifying words—then saving the
    changes. The output will be reviewed to ensure that edits are
    accurately reflected and retained.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:}User logged in and has uploaded at least one
    image with generated alt text.\\
    {\bf Input:} Upload multiple images sequentially within the same
    session, then navigate through the session interface.\\
    {\bf Output:} All previously uploaded images and their
    corresponding alt texts remain visible and accessible until the
    user logs out or the session ends.\\
    {\bf  Case Derivation:}According to \textbf{FR 5} criterion, all
    uploaded images and their corresponding alternative texts should
    remain visible within the same session. Therefore, the expected
    outcome is that users can access and review all prior uploads
    without reloading or re-uploading them during an active session.\\
    {\bf How test will be performed:} The test can be performed by
    uploading multiple images within the same session, then
    navigating across different pages or refreshing the interface.
    The test will verify that all uploaded images and their
    corresponding alternative texts remain visible until the session ends.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:} Login page displayed.\\
    {\bf Input:} Access for login is defined below:
    \begin{itemize}
      \item Valid credentials: McMaster University email and password
      \item Invalid credentials: non-McMaster email or incorrect password
    \end{itemize}
    {\bf Output:} Access granted only to users with valid McMaster
    credentials. Invalid attempts are rejected with an appropriate
    error message (e.g., “Invalid login credentials”).\\
    {\bf  Case Derivation:}As outlined in the \textbf{FR 6}
    criterion, only users with verified McMaster University
    credentials should gain access to system features. The expected
    outcome is that valid users can log in successfully, while
    unauthorized or invalid attempts are denied with an appropriate
    error message.\\
    {\bf How test will be performed:} The test can be performed by
    attempting to log in using both valid McMaster University
    credentials and invalid credentials. The system’s behavior will
    be reviewed to confirm that only verified users gain access,
    while invalid attempts produce an appropriate error message.\\
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

{Need a small summary blurb here pls. - NF}

\paragraph{Tests for Non-Functional Requirements}

\begin{enumerate}[label=NFR-ST \arabic*., wide=0pt, leftmargin=*]

  \item{}
    \textbf{Text Resizing and Contrast Accessibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} LFR-AR1, LFR-AR2, LFR-AR3, LFR-AR4. \\
    \textbf{Initial State:} Interface displayed in a standard browser
    with accessibility tools enabled. \\
    \textbf{Input/Condition:} The user adjusts browser zoom to the
    maximum allowed and reviews color usage. \\
    \textbf{Output/Result:} Text resizes correctly without overlap;
    information is not conveyed by color alone; contrast meets
    accessibility thresholds; all images have alternative text. \\[2mm]
    \textbf{How test will be performed:} The tester will manually
    adjust zoom levels, use color contrast tools, and run screen
    reader tests to confirm accessibility compliance.

  \item \textbf{Interface Style and Branding Verification}. \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} LFR-SR1, LFR-SR2, LFR-SR3. \\
    \textbf{Initial State:} System interface displayed in a browser. \\
    \textbf{Input/Condition:} Visual inspection of layout, font,
    colors, branding, and adherence to Norman's design principles. \\
    \textbf{Output/Result:} Interface maintains modern and simple
    style, McMaster branding is present without interfering with
    usability, and design elements comply with Norman’s principles. \\[2mm]
    \textbf{How test will be performed:} Tester will review the
    interface against the style guide, branding requirements, and
    Norman’s design checklist to ensure compliance.

  \item \textbf{Usability Efficiency and Learnability}. \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} UHR-EUR1, UHR-EUR2, UHR-EUR3, UHR-EUR4 \\
    \textbf{Initial State:} System interface available to first-time
    and returning users \\
    \textbf{Input/Condition:} Users perform key actions: login,
    upload images, generate alt text \\
    \textbf{Output/Result:} Users complete tasks efficiently, recall
    steps after a break, receive feedback within 1 second, and can
    correct errors easily \\[2mm]
    \textbf{How test will be performed:} Conduct usability sessions
    with participants performing all major tasks while timing
    actions, recording feedback response, and monitoring error recovery.

  \item \textbf{Alt Text Storage and Personalization Options} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-PIR1 \\
    \textbf{Initial State:} Alt text generated for uploaded image \\
    \textbf{Input/Condition:} User chooses to copy or download
    generated alt text \\
    \textbf{Output/Result:} Alt text is successfully copied to
    clipboard or downloaded as .txt \\[2mm]
    \textbf{How test will be performed:} Tester generates alt text
    and selects each option, confirming that the system executes the
    chosen storage method correctly.

  \item \textbf{Accessibility for Screen Readers and Low-Vision Users} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-LR1, UHR-AR1, UHR-AR2 \\
    \textbf{Initial State:} Interface accessible with popular screen readers \\
    \textbf{Input/Condition:} Users with screen readers upload images
    and generate alt text \\
    \textbf{Output/Result:} Users successfully generate alt text
    within the time limit and can navigate interface efficiently \\[2mm]
    \textbf{How test will be performed:} Conduct sessions with
    low-vision participants using NVDA, JAWS, or VoiceOver and
    measure task completion time and success rates.

  \item \textbf{Information Display Clarity} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-LR2 \\
    \textbf{Initial State:} Interface displays system messages and information \\
    \textbf{Input/Condition:} User navigates through tasks without
    developer guidance \\
    \textbf{Output/Result:} Only essential information is displayed;
    technical details are hidden \\[2mm]
    \textbf{How test will be performed:} Tester inspects interface
    during task completion to confirm that technical or unnecessary
    details are not visible to the user.

  \item \textbf{Performance – Alt Text Generation Time} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-SL1, PR-SL2 \\
    \textbf{Initial State:} System under typical load conditions \\
    \textbf{Input/Condition:} Upload images of various sizes (small and large) \\
    \textbf{Output/Result:} Generated alt text returned within
    specified thresholds for each image size; UI responds within
    T\_UI\_RESP \\[2mm]
    \textbf{How test will be performed:} Automated scripts upload
    images and record generation time and UI response time; results
    plotted to verify performance meets criteria.

  \item \textbf{Safety and Timeout Handling} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-SR-HA1, PR-SR-HA2, PR-SR-HA3 \\
    \textbf{Initial State:} System ready for alt text generation \\
    \textbf{Input/Condition:} Simulate long-running or stalled image
    processing \\
    \textbf{Output/Result:} User notified of timeout, option to
    retry; incomplete data is deleted; messages do not reveal
    technical details \\ [2mm]
    \textbf{How test will be performed:} Tester simulates timeouts
    and verifies notifications, data cleanup, and absence of
    technical information in messages.

  \item \textbf{Alt Text Accuracy and Usability Evaluation} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-PAR1, PR-PAR2, PR-PAR3, CR-LR1 \\
    \textbf{Initial State:} Alt text generated for uploaded images \\
    \textbf{Input/Condition:} Users rate generated alt text for
    sufficiency, length, readability, and usability \\
    \textbf{Output/Result:} Ratings meet R\_SUFFICIENCY, R\_LENGTH, and
    R\_USABILITY\_MEDIAN thresholds \\[2mm]
    \textbf{How test will be performed:} Conduct structured user
    evaluation using rating scales; calculate statistics to confirm
    compliance with quality thresholds.

  \item \textbf{Robustness to Invalid Inputs and Fault Recovery} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} PR-RFT1, PR-RFT2 \\
    \textbf{Initial State:} System running normally \\
    \textbf{Input/Condition:} Upload unsupported, corrupted files, or
    simulate backend process failures \\
    \textbf{Output/Result:} Clear error messages displayed; system recovers within T\_RECOVERY \\[2mm]
    \textbf{How test will be performed:} Tester uploads invalid files
    and observes error handling; automated test simulates isolated
    failures to confirm automatic recovery.

  \item \textbf{Concurrent Usage and Storage Capacity} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR1, PR-CR2 \\
    \textbf{Initial State:} System deployed in test environment \\
    \textbf{Input/Condition:} Simulate multiple simultaneous users
    and upload datasets \\
    \textbf{Output/Result:} Supports CAP\_CONCURRENT users with
    response times $\le$ 10s; storage handles CAP\_STORAGE datasets \\[2mm]
    \textbf{How test will be performed:} Load testing scripts
    simulate multiple concurrent requests and dataset uploads;
    performance and storage usage monitored.

  \item \textbf{System Extensibility and Maintainability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-SER1, PR-LR1, PR-LR2, MS-MNT1, MS-MNT2,
    MS-MNT3, MS-AD1, MS-AD2, MS-AD3 \\
    \textbf{Initial State:} Existing modular system codebase deployed \\
    \textbf{Input/Condition:} Apply updates to modules,
    configuration, or AI models \\
    \textbf{Output/Result:} System maintains functionality; new
    modules integrate without breaking existing components; changes
    tracked \\[2mm]
    \textbf{How test will be performed:} Tester modifies components
    and configuration files, runs automated CI/CD tests, and verifies
    integration of new modules.

  \item \textbf{Security, Access, and Network Restrictions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} SR-AR1, SR-AR2, SR-IR1, SR-IR2, SR-PR1, SR-PR2,
    SR-AU1, SR-AU2, SR-IM1, SR-IM2 \\
    \textbf{Initial State:} System deployed with SSO and HTTPS enabled \\
    \textbf{Input/Condition:} Attempt unauthorized access, upload
    images, and inspect logs \\
    \textbf{Output/Result:} Only authorized McMaster users gain
    access; encrypted communication enforced; uploaded images
    deleted; PII filtered; logs access restricted; unsupported files
    rejected; external networks blocked \\[2mm]
    \textbf{How test will be performed:} Testers attempt invalid
    logins, inspect encrypted traffic, upload images and check
    deletion, validate moderation filters, and verify network
    restrictions and audit log access.

  \item \textbf{Cultural and Professional Content Compliance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} CR1, CR2, CR3 \\
    \textbf{Initial State:} Alt text generated by the system \\
    \textbf{Input/Condition:} Review generated alt text samples \\
    \textbf{Output/Result:} Text is neutral, inclusive, contextually
    accurate, and professional \\[2mm]
    \textbf{How test will be performed:} Tester inspects a variety of
    outputs for bias, unnecessary cultural references, and tone appropriateness.

  \item \textbf{Compliance and Regulatory Verification} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} CR-LR1, CR-SCR1, CR-SCR2 \\
    \textbf{Initial State:} System generates alt text and manages
    uploaded data \\
    \textbf{Input/Condition:} Validate against AODA, WCAG 2.1, and
    institutional privacy policies \\
    \textbf{Output/Result:} Generated alt text meets accessibility
    standards; uploaded files handled per policy; documentation
    available for stakeholders \\[2mm]
    \textbf{How test will be performed:} Run accessibility and
    privacy compliance tests; inspect system logs and documentation
    for adherence.

  \item \textbf{Environmental and Device Compatibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-EP1, OER-EP2, OER-WE1, OER-WE2 \\
    \textbf{Initial State:} System installed on multiple devices and
    OS platforms \\
    \textbf{Input/Condition:} Access system in standard classroom,
    office, or cloud environments \\
    \textbf{Output/Result:} System functions reliably across devices,
    platforms, and typical indoor conditions; maintains network
    connectivity \\[2mm]
    \textbf{How test will be performed:} Testers access the system on
    Windows, Mac, and Linux with various browsers; verify full functionality.

  \item \textbf{Documentation and Logging Availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} MS-LOG1, MS-LOG2, MS-LOG3 \\
    \textbf{Initial State:} System operational with logging enabled \\
    \textbf{Input/Condition:} Generate alt text and simulate user actions \\
    \textbf{Output/Result:} Comprehensive logs maintained;
    documentation available for developers and users; logs do not
    contain PII \\[2mm]
    \textbf{How test will be performed:} Tester performs sample
    actions; inspects logs and documentation for completeness and
    privacy compliance.

  \item \textbf{Error Handling and Feedback Clarity} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-RFT3, PR-RFT4 \\
    \textbf{Initial State:} System running normally \\
    \textbf{Input/Condition:} Trigger recoverable and non-recoverable errors \\
    \textbf{Output/Result:} Clear, concise, and non-technical error
    messages displayed; user able to recover or retry \\[2mm]
    \textbf{How test will be performed:} Tester induces known errors
    (e.g., network failure, invalid file upload) and verifies
    displayed messages and recovery options.

  \item \textbf{Data Retention and Auto-Deletion Compliance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} SR-DTR1, SR-DTR2 \\
    \textbf{Initial State:} User data uploaded and processed \\
    \textbf{Input/Condition:} System retains data past defined
    retention period \\
    \textbf{Output/Result:} Data automatically deleted per policy; no
    unauthorized access possible \\[2mm]
    \textbf{How test will be performed:} Tester checks timestamped
    data and verifies auto-deletion after expiration; attempts access
    post-deletion to confirm security.

  \item \textbf{System Logging and Audit Trail} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} MS-LOG4, MS-LOG5 \\
    \textbf{Initial State:} System operational with logging enabled \\
    \textbf{Input/Condition:} Perform multiple system operations and
    user logins \\
    \textbf{Output/Result:} All critical actions logged securely;
    logs immutable and auditable \\[2mm]
    \textbf{How test will be performed:} Tester executes operations
    and reviews logs for completeness, accuracy, and security compliance.

  \item \textbf{System Recovery and Failover} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} PR-RFT5, PR-RFT6 \\
    \textbf{Initial State:} System operating under normal load \\
    \textbf{Input/Condition:} Simulate server crash, database
    failure, or network outage \\
    \textbf{Output/Result:} System recovers automatically; user
    experience minimally impacted; data integrity maintained \\[2mm]
    \textbf{How test will be performed:} Tester simulates failures
    and verifies recovery procedures, system availability, and data integrity.

  \item \textbf{AI Model Update and Version Control} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-SER2, MS-MNT4 \\
    \textbf{Initial State:} AI model deployed with version control \\
    \textbf{Input/Condition:} Apply new model updates \\
    \textbf{Output/Result:} New model integrates without breaking
    functionality; rollback possible if necessary \\[2mm]
    \textbf{How test will be performed:} Tester deploys updates in
    staging environment and verifies system functions; tests rollback procedure.

  \item \textbf{Backup and Restore Functionality} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-RST1, PR-RST2 \\
    \textbf{Initial State:} System operational with recent backups \\
    \textbf{Input/Condition:} Restore system from backup after
    simulated failure \\
    \textbf{Output/Result:} System restores to prior state; no data
    loss beyond last backup \\[2mm]
    \textbf{How test will be performed:} Tester triggers backup
    restoration and verifies system state and data consistency.

  \item \textbf{Load Handling and Stress Testing} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR3, PR-CR4 \\
    \textbf{Initial State:} System deployed with baseline load \\
    \textbf{Input/Condition:} Simulate increasing concurrent users
    and transactions beyond expected peak \\
    \textbf{Output/Result:} System maintains acceptable response
    times and throughput; no critical failures \\[2mm]
    \textbf{How test will be performed:} Automated scripts generate
    load; monitor CPU, memory, and response times; confirm thresholds
    not exceeded.

  \item \textbf{Cross-Browser Compatibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-BC1, OER-BC2 \\
    \textbf{Initial State:} System accessible from modern browsers \\
    \textbf{Input/Condition:} Access system via Chrome, Edge,
    Firefox, and Safari \\
    \textbf{Output/Result:} Interface renders correctly;
    functionality consistent across browsers \\[2mm]
    \textbf{How test will be performed:} Tester manually accesses
    system from each browser and verifies UI, functionality, and performance.

  \item \textbf{Mobile Responsiveness} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-MR1, OER-MR2 \\
    \textbf{Initial State:} System deployed in browser and mobile view \\
    \textbf{Input/Condition:} Access system from smartphones and
    tablets of different resolutions \\
    \textbf{Output/Result:} UI scales correctly; touch interactions
    functional; alt text generation accessible \\[2mm]
    \textbf{How test will be performed:} Tester uses multiple devices
    and orientations, verifying layout, functionality, and accessibility.

  \item \textbf{Data Encryption in Transit and Storage} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-PR3, SR-PR4 \\
    \textbf{Initial State:} System with HTTPS and encrypted database enabled \\
    \textbf{Input/Condition:} Upload images and generate alt text \\
    \textbf{Output/Result:} All data encrypted during transit and
    storage; unauthorized decryption prevented \\[2mm]
    \textbf{How test will be performed:} Automated scripts intercept
    and attempt decryption; confirm encryption standards met.

  \item \textbf{Accessibility Compliance Documentation} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} CR-LR2, CR-LR3 \\
    \textbf{Initial State:} Documentation available in project repository \\
    \textbf{Input/Condition:} Review compliance documentation for
    WCAG 2.1 and AODA \\
    \textbf{Output/Result:} Documentation fully describes
    accessibility features, test results, and compliance levels \\[2mm]
    \textbf{How test will be performed:} Tester reads documentation
    and confirms all standards, features, and results documented accurately.

  \item \textbf{Scalability of Storage and Processing} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR5, PR-CR6 \\
    \textbf{Initial State:} System running with normal load \\
    \textbf{Input/Condition:} Upload datasets with sizes increasing
    up to 10x expected usage \\
    \textbf{Output/Result:} System handles scaling without
    degradation; storage and processing remain within thresholds \\[2mm]
    \textbf{How test will be performed:} Automated scripts simulate
    large datasets; monitor CPU, memory, and response times; confirm
    thresholds not exceeded.

  \item \textbf{Session Management and Timeout Enforcement} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-AR3, SR-AR4 \\
    \textbf{Initial State:} User logged in \\
    \textbf{Input/Condition:} Remain idle for session timeout period \\
    \textbf{Output/Result:} User automatically logged out; session
    data cleared \\[2mm]
    \textbf{How test will be performed:} Tester leaves session idle
    and verifies automatic logout and data clearance.

  \item \textbf{Internationalization and Localization} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-LC1, OER-LC2 \\
    \textbf{Initial State:} Interface configured with language options \\
    \textbf{Input/Condition:} Switch language settings to French,
    Spanish, or other supported languages \\
    \textbf{Output/Result:} UI, messages, and alt text generation
    output properly localized \\[2mm]
    \textbf{How test will be performed:} Tester changes language
    settings and inspects all interface components and generated alt text.

  \item \textbf{Logging Level and Error Traceability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} MS-LOG6, MS-LOG7 \\
    \textbf{Initial State:} System logging configured \\
    \textbf{Input/Condition:} Trigger minor, major, and critical errors \\
    \textbf{Output/Result:} Logs record errors with severity and
    timestamp; traceability from log to incident possible \\[2mm]
    \textbf{How test will be performed:} Tester induces errors and
    reviews logs for completeness, severity, and traceability.

  \item \textbf{Maintainability – Code Documentation} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} MS-MNT5, MS-MNT6 \\
    \textbf{Initial State:} System code deployed \\
    \textbf{Input/Condition:} Review code comments, README, and
    module documentation \\
    \textbf{Output/Result:} Documentation sufficient for new
    developer onboarding; functions and classes clearly described \\[2mm]
    \textbf{How test will be performed:} Tester reviews repository,
    checks code comments and documentation quality, and attempts to
    understand functionality.

  \item \textbf{Reliability and Uptime Metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-REL1, PR-REL2 \\
    \textbf{Initial State:} System deployed in production environment \\
    \textbf{Input/Condition:} Continuous operation over 30 days \\
    \textbf{Output/Result:} Uptime $\ge$ 99.5 percentage; critical errors $\le$ 0.5 percentage \\[2mm]
    \textbf{How test will be performed:} Automated monitoring scripts
    track system availability, error rates, and generate uptime reports.

  \item \textbf{Audit and Regulatory Reporting} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} CR-SCR3, CR-SCR4 \\
    \textbf{Initial State:} System operational with logging and data
    retention policies \\
    \textbf{Input/Condition:} Generate audit report \\
    \textbf{Output/Result:} Report includes user access, data
    retention, and compliance with standards \\[2mm]
    \textbf{How test will be performed:} Tester generates reports and
    verifies completeness and compliance with regulatory standards.

  \item \textbf{Usability Feedback Collection} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-FB1, UHR-FB2 \\
    \textbf{Initial State:} System includes feedback collection mechanisms \
    \textbf{Input/Condition:} Users submit feedback after alt text generation \\
    \textbf{Output/Result:} Feedback stored and categorized; system
    able to analyze usability trends \\[2mm]
    \textbf{How test will be performed:} Tester submits sample
    feedback and verifies storage, categorization, and accessibility
    of feedback for review.

  \item \textbf{System Responsiveness Under Load} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR7, PR-CR8 \\
    \textbf{Initial State:} System operational with normal load \\
    \textbf{Input/Condition:} Increase concurrent user activity to 2x
    expected peak \\
    \textbf{Output/Result:} UI response time $\le$ 3s for 95 percentage of requests. \\[2mm]
    \textbf{How test will be performed:} Automated load scripts
    simulate multiple users; system response times recorded and analyzed.

  \item \textbf{Third-Party API Integration Stability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} PR-API1, PR-API2 \\
    \textbf{Initial State:} System connected to external APIs \\
    \textbf{Input/Condition:} Perform repeated alt text generation requests \\
    \textbf{Output/Result:} API responses consistent; errors handled
    gracefully; system continues normal operation \\[2mm]
    \textbf{How test will be performed:} Tester triggers repeated API
    calls and monitors system stability, error handling, and response
    consistency.

  \item \textbf{User Interface Localization Accuracy} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-LC3, OER-LC4 \\
    \textbf{Initial State:} Multi-language system interface \\
    \textbf{Input/Condition:} Switch to each supported language \\
    \textbf{Output/Result:} All messages, labels, and generated
    content correctly localized; no truncation or overlap \\[2mm]
    \textbf{How test will be performed:} Tester switches language
    settings and inspects UI and content for correctness and readability.

  \item \textbf{Cross-Platform File Upload Compatibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-FC1, OER-FC2 \\
    \textbf{Initial State:} System accessible from Windows, Mac, Linux \\
    \textbf{Input/Condition:} Upload images from each platform \\
    \textbf{Output/Result:} All images successfully uploaded and
    processed; no format-specific errors \\[2mm]
    \textbf{How test will be performed:} Tester uploads images from
    each platform and verifies correct processing.

  \item \textbf{Accessibility Feature Toggle Verification} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AR5, LFR-AR6 \\
    \textbf{Initial State:} Accessibility features available \\
    \textbf{Input/Condition:} Enable and disable accessibility
    options such as high-contrast mode and text resizing \\
    \textbf{Output/Result:} Features function as intended without
    affecting core operations \\[2mm]
    \textbf{How test will be performed:} Tester toggles options and
    confirms UI adapts correctly and tasks remain fully functional.

  \item \textbf{Session Persistence Across Devices} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} SR-SES1, SR-SES2 \\
    \textbf{Initial State:} User logged in on one device \\
    \textbf{Input/Condition:} Log in from a second device \\
    \textbf{Output/Result:} Session persists correctly; no data loss
    or overlap occurs \\[2mm]
    \textbf{How test will be performed:} Tester logs in from multiple
    devices and verifies session continuity and data integrity.

  \item \textbf{Authentication and Authorization Robustness} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-SEC1, SR-SEC2 \\
    \textbf{Initial State:} System user management enabled \\
    \textbf{Input/Condition:} Attempt login with valid and invalid
    credentials, and role-based access attempts \\
    \textbf{Output/Result:} Only authorized users gain access; failed
    attempts logged and blocked \\[2mm]
    \textbf{How test will be performed:} Tester executes login
    attempts and role-specific actions, verifying proper access
    control and logging.

  \item \textbf{Backup Frequency and Integrity Verification} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-RST3, PR-RST4 \\
    \textbf{Initial State:} System operational with scheduled backups \\
    \textbf{Input/Condition:} Check backup creation and integrity
    after scheduled intervals \\
    \textbf{Output/Result:} Backup files are created, complete, and
    restorable \\[2mm]
    \textbf{How test will be performed:} Automated scripts validate
    backup presence, completeness, and successful restore operations.

  \item \textbf{System Latency under Concurrent Requests} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR9, PR-CR10 \\
    \textbf{Initial State:} System operating normally \\
    \textbf{Input/Condition:} Simulate multiple concurrent requests
    exceeding peak load \\
    \textbf{Output/Result:} Average latency remains below 2 seconds
    for 95% of requests \\[2mm]
    \textbf{How test will be performed:} Automated load testing
    scripts measure response times and record metrics.

  \item \textbf{Accessibility Feature Consistency Across Pages} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AR7, LFR-AR8 \\
    \textbf{Initial State:} System accessible via web interface \\
    \textbf{Input/Condition:} Navigate through all interface pages
    while accessibility features enabled \\
    \textbf{Output/Result:} All pages reflect selected accessibility
    settings consistently \\[2mm]
    \textbf{How test will be performed:} Tester manually navigates
    pages and confirms consistent application of accessibility features.

  \item \textbf{Data Validation Accuracy} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-DV1, SR-DV2 \\
    \textbf{Initial State:} System receiving data input \\
    \textbf{Input/Condition:} Submit valid and invalid data sets \\
    \textbf{Output/Result:} Invalid data rejected with proper
    messages; valid data accepted correctly \\[2mm]
    \textbf{How test will be performed:} Automated tests feed data
    and verify validation, error messages, and acceptance of correct entries.

  \item \textbf{System Performance Logging} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} MS-LOG8, MS-LOG9 \\
    \textbf{Initial State:} System operational with monitoring enabled \\
    \textbf{Input/Condition:} Conduct normal and peak operations \\
    \textbf{Output/Result:} Performance metrics recorded accurately;
    logs available for analysis \\[2mm]
    \textbf{How test will be performed:} Automated scripts simulate
    usage and check log accuracy, completeness, and integrity.

  \item \textbf{System Maintainability Through Modular Design} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} MS-MNT7, MS-MNT8 \\
    \textbf{Initial State:} System code deployed \\
    \textbf{Input/Condition:} Review code modules and interdependencies \\
    \textbf{Output/Result:} Modules decoupled; maintenance tasks
    simplified; documentation available \\[2mm]
    \textbf{How test will be performed:} Tester inspects codebase and
    verifies module independence and clear documentation.

  \item \textbf{System Monitoring Dashboard Functionality} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-MON1, PR-MON2 \\
    \textbf{Initial State:} Monitoring dashboard active \\
    \textbf{Input/Condition:} Generate alerts and simulate metric spikes \\
    \textbf{Output/Result:} Dashboard reflects accurate status and
    metrics in real-time \\[2mm]
    \textbf{How test will be performed:} Tester triggers alerts and
    monitors dashboard for correct visualization and metrics.

  \item \textbf{Accessibility Compliance Reporting} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} CR-LR4, CR-LR5 \\
    \textbf{Initial State:} Accessibility features and logging enabled \\
    \textbf{Input/Condition:} Generate compliance report \\
    \textbf{Output/Result:} Report shows coverage of WCAG 2.1
    standards; any non-compliance highlighted \\[2mm]
    \textbf{How test will be performed:} Tester reviews reports and
    validates against accessibility standards checklist.

  \item \textbf{Data Privacy and GDPR Compliance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} SR-PR5, SR-PR6 \\
    \textbf{Initial State:} User data stored \\
    \textbf{Input/Condition:} Access and delete personal data \\
    \textbf{Output/Result:} Personal data protected and deletions
    effective; no unauthorized access \\[2mm]
    \textbf{How test will be performed:} Tester attempts access and
    deletion operations, verifying compliance with privacy standards.

  \item \textbf{High Availability Through Redundancy} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-REL3, PR-REL4 \\
    \textbf{Initial State:} System deployed with redundant servers \\
    \textbf{Input/Condition:} Simulate server failure \\
    \textbf{Output/Result:} System continues operation with minimal
    downtime; failover verified \\[2mm]
    \textbf{How test will be performed:} Tester disables primary
    server and observes continuity of service.

  \item \textbf{Notification Delivery and Logging} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-NOT1, PR-NOT2 \\
    \textbf{Initial State:} Notification system active \\
    \textbf{Input/Condition:} Trigger various system notifications \\
    \textbf{Output/Result:} Notifications sent promptly; logged and
    traceable \\[2mm]
    \textbf{How test will be performed:} Tester triggers
    notifications and verifies delivery, logging, and accuracy.

  \item \textbf{Accessibility User Preferences Persistence} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AR9, LFR-AR10 \\
    \textbf{Initial State:} Accessibility preferences set by user \\
    \textbf{Input/Condition:} Log out and log back in \\
    \textbf{Output/Result:} Preferences persist and applied automatically \\[2mm]
    \textbf{How test will be performed:} Tester logs out/in and
    verifies consistent application of user accessibility settings.

  \item \textbf{Version Control and Deployment Audit} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} MS-MNT9, MS-MNT10 \\
    \textbf{Initial State:} System deployed with version control \\
    \textbf{Input/Condition:} Review deployment history \\
    \textbf{Output/Result:} All deployments documented; rollback
    history available \\[2mm]
    \textbf{How test will be performed:} Tester inspects deployment
    logs and confirms version tracking and rollback capability.

  \item \textbf{Image Processing Throughput} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR11, PR-CR12 \\
    \textbf{Initial State:} System idle \\
    \textbf{Input/Condition:} Upload batch of images for alt text generation \\
    \textbf{Output/Result:} All images processed within expected time
    threshold \\[2mm]
    \textbf{How test will be performed:} Automated tests measure
    processing time for large image batches.

  \item \textbf{System Health Check Automation} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-MON3, PR-MON4 \\
    \textbf{Initial State:} System running \\
    \textbf{Input/Condition:} Scheduled automated health checks \\
    \textbf{Output/Result:} Alerts generated for any anomalies;
    uptime maintained \\[2mm]
    \textbf{How test will be performed:} Automated health scripts
    execute and log system metrics, notifying tester on anomalies.

  \item \textbf{User Onboarding Accessibility Guidance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AR11, LFR-AR12 \\
    \textbf{Initial State:} New user account created \\
    \textbf{Input/Condition:} User accesses onboarding tutorial \\
    \textbf{Output/Result:} Tutorial accessible via screen reader and
    keyboard navigation \\[2mm]
    \textbf{How test will be performed:} Tester follows tutorial
    using accessibility tools and verifies all steps are readable and navigable.

  \item \textbf{Data Consistency Across Multiple Instances} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-DV3, SR-DV4 \\
    \textbf{Initial State:} System running on multiple instances \\
    \textbf{Input/Condition:} Modify data concurrently \\
    \textbf{Output/Result:} Data remains consistent across all instances \\[2mm]
    \textbf{How test will be performed:} Automated tests simulate
    concurrent data operations and verify consistency.

  \item \textbf{Accessibility Keyboard Shortcut Support} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AR13, LFR-AR14 \\
    \textbf{Initial State:} System accessible via keyboard \\
    \textbf{Input/Condition:} Navigate and perform tasks using only keyboard \\
    \textbf{Output/Result:} All interface elements operable; focus
    indicators clear \\[2mm]
    \textbf{How test will be performed:} Tester completes tasks
    without mouse and verifies navigation and operation.

  \item \textbf{End-to-End Encryption Verification} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} SR-PR7, SR-PR8 \\
    \textbf{Initial State:} Data transfer enabled \\
    \textbf{Input/Condition:} Transmit sensitive data \\
    \textbf{Output/Result:} Data encrypted in transit and decrypted
    only by intended recipient \\[2mm]
    \textbf{How test will be performed:} Automated tests attempt
    interception; validate encryption and decryption correctness.

  \item \textbf{System Scalability for Concurrent Users} \\2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR13, PR-CR14 \\
    \textbf{Initial State:} System running normally \\
    \textbf{Input/Condition:} Simulate up to 5x expected user load \\
    \textbf{Output/Result:} System handles load without significant
    performance degradation \\[2mm]
    \textbf{How test will be performed:} Automated load scripts
    generate peak traffic; monitor system response and resource usage.

  \item \textbf{Comprehensive Audit Trail Accessibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} MS-LOG10, MS-LOG11 \\
    \textbf{Initial State:} System logs active \\
    \textbf{Input/Condition:} Access audit trail \\
    \textbf{Output/Result:} Logs readable, searchable, and meet
    regulatory standards \\[2mm]
    \textbf{How test will be performed:} Tester queries audit trail
    and confirms completeness and accessibility.

  \item \textbf{Image Alt Text Accuracy Metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-VAL1, PR-VAL2 \\
    \textbf{Initial State:} Alt text generation operational \\
    \textbf{Input/Condition:} Process test image set \\
    \textbf{Output/Result:} Accuracy metrics calculated; performance
    within target thresholds \\[2mm]
    \textbf{How test will be performed:} Automated evaluation against
    ground truth alt text; calculate accuracy, precision, and recall.

  \item \textbf{End-to-End System Security Review} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\
    \textbf{Covers:} SR-SEC3, SR-SEC4 \\
    \textbf{Initial State:} System deployed in production \\
    \textbf{Input/Condition:} Conduct security assessment \\
    \textbf{Output/Result:} All vulnerabilities identified; security
    policies validated \\[2mm]
    \textbf{How test will be performed:} Tester reviews system
    architecture, performs penetration tests, and verifies adherence
    to security standards.\\

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
requirements.}

\section{Unit Test Description}
\label{Unit Test Desc}


\subsection{Unit Testing Scope}

The purpose of unit testing for \textit{Reading4All} is to verify the correctness, robustness, and accessibility compliance of individual components prior to system integration. Each module is tested independently using automated test scripts (PyTest) and deterministic input fixtures.

\textbf{Modules in Scope:}
\begin{itemize}
  \item Image Upload \& Validation Module
  \item Alt-Text Generation Module
  \item Accessibility and UI Compliance Module
  \item Security and Privacy Module
\end{itemize}

\textbf{Modules Out of Scope:}
Third-party OCR engines, pretrained vision/language models, and external McMaster authentication services are assumed to be validated independently. Only the thin wrappers and internal interactions with these APIs are tested here.

\subsection{Tests for Functional Requirements}

\subsubsection{Module 1 \textemdash{} Image Upload and Validation}

\textbf{Goal:} Ensure uploaded images meet all input constraints for format, size, and type, and that invalid files are rejected gracefully.

\begin{enumerate}
\item{UT1-UploadValidImage\\}
Type: Functional, Dynamic, Automatic\\
Initial State: Application running; no active uploads.\\
Input: Valid PNG image, 1 MB in size.\\
Output: File accepted; confirmation message displayed; metadata stored in temporary session.\\
Test Case Derivation: Confirms compliance with input constraints (JPEG/PNG $\leq$ 10 MB).\\
How test will be performed: Run automated pytest verifying HTTP 200 response and valid JSON schema.

\item{UT2-UploadInvalidFileType\\}
Type: Functional, Dynamic, Automatic\\
Initial State: No uploads.\\
Input: Unsupported file type (e.g., \texttt{.pdf}).\\
Output: Error message ``Unsupported file format'' returned; no data stored.\\
Test Case Derivation: Validates enforcement of file type constraint and secure rejection.\\
How test will be performed: Send POST request with invalid MIME type; assert error 400 and log entry.

\item{UT3-UploadOversizedFile\\}
Type: Functional, Dynamic, Automatic\\
Initial State: No uploads.\\
Input: PNG file $>$ 10 MB.\\
Output: Upload rejected with clear error; no file stored.\\
Test Case Derivation: Confirms handling of maximum size threshold.\\
How test will be performed: Simulate multipart upload; verify memory cleanup and error alert.
\end{enumerate}

\subsubsection{Module 2 \textemdash{} Alt-Text Generation}

\textbf{Goal:} Validate that image inference and text generation components produce deterministic, relevant, and correctly formatted alternative text.

\begin{enumerate}
\item{UT4-GenerateAltText\\}
Type: Functional, Dynamic, Automatic\\
Initial State: Valid image uploaded and accessible to model service.\\
Input: Image containing labeled diagram.\\
Output: Non-empty descriptive string within 3--8~s latency window.\\
Test Case Derivation: Confirms compliance with generation timing and content sufficiency.\\
How test will be performed: Mock ML service; assert response schema and timing $<$ T\_ALT\_GEN\_SMALL.

\item{UT5-EditAltText\\}
Type: Functional, Dynamic, Manual\\
Initial State: Alt-text successfully generated.\\
Input: User edits description and saves.\\
Output: Edited text replaces old version in session storage.\\
Test Case Derivation: Ensures edit functionality modifies session data only.\\
How test will be performed: Selenium automation of UI; assert saved value persists on reload.

\item{UT6-HandleEmptyAltText\\}
Type: Functional, Dynamic, Automatic\\
Initial State: Image uploaded yields no model output.\\
Input: Blank model return.\\
Output: Error message and retry option; no text stored.\\
Test Case Derivation: Confirms graceful failure and user notification.\\
How test will be performed: Patch model API to return empty string; validate error log.
\end{enumerate}

\subsubsection{Module 3 \textemdash{} Accessibility and UI Compliance}

\textbf{Goal:} Validate that all UI components meet accessibility and usability criteria (keyboard navigation, zoom, color contrast, alt text labeling).

\begin{enumerate}
\item{UT7-KeyboardNavigation\\}
Type: Functional, Dynamic, Automatic\\
Initial State: Application home screen loaded.\\
Input: Simulated Tab and Enter key presses.\\
Output: All focusable elements reachable; no trap detected.\\
Test Case Derivation: Confirms WCAG 2.1 Success Criterion 2.1.1.\\
How test will be performed: Automated Axe/WAVE accessibility scan with keyboard simulation.

\item{UT8-ContrastValidation\\}
Type: Functional, Static, Automatic\\
Initial State: Deployed UI snapshot available.\\
Input: CSS stylesheet.\\
Output: All color pairs $\geq$ 4.5:1 contrast ratio.\\
Test Case Derivation: Confirms MIN\_CONTRAST\_RATIO threshold met.\\
How test will be performed: Run Lighthouse CI contrast-check script.

\item{UT9-ZoomResilience\\}
Type: Functional, Manual\\
Initial State: Browser window at 100\%.\\
Input: Zoom increased to 200\%.\\
Output: Interface remains fully visible and interactive.\\
Test Case Derivation: Ensures compliance with MAX\_ZOOM\_PERCENTAGE.\\
How test will be performed: Manual inspection + screen-reader pass.
\end{enumerate}

\subsubsection{Module 4 \textemdash{} Security and Privacy}

\textbf{Goal:} Verify that all authentication, encryption, and data-deletion procedures uphold confidentiality and integrity requirements.

\begin{enumerate}
\item{UT10-LoginAuthentication\\}
Type: Functional, Dynamic, Automatic\\
Initial State: No user session.\\
Input: Valid McMaster SSO credentials.\\
Output: Access granted; session token stored.\\
Test Case Derivation: Ensures access restriction and session linking.\\
How test will be performed: Mock OAuth SSO; assert 200 and JWT valid.

\item{UT11-RejectUnauthorizedAccess\\}
Type: Functional, Dynamic, Automatic\\
Initial State: No valid session token.\\
Input: API request to /generate endpoint.\\
Output: HTTP 401 Unauthorized.\\
Test Case Derivation: Confirms secure access control.\\
How test will be performed: Post request without auth header; verify denial and log entry.

\item{UT12-TemporaryFileDeletion\\}
Type: Functional, Dynamic, Automatic\\
Initial State: Completed alt-text generation.\\
Input: Wait $>$ 60~s.\\
Output: Uploaded file deleted from temporary directory.\\
Test Case Derivation: Verifies privacy compliance (FILE\_DELETE\_TIME).\\
How test will be performed: Check directory contents before/after timeout.
\end{enumerate}

\subsection{Tests for Non-Functional Requirements}

\subsubsection{Module 5 \textemdash{} Performance and Reliability}

\textbf{Goal:} Ensure responsiveness, stability, and error handling meet defined thresholds.

\begin{enumerate}
\item{UT13-LatencyBenchmark\\}
Type: Dynamic, Automatic\\
Input/Condition: Upload 5 images $\leq$ 2~MB each concurrently.\\
Output/Result: Mean response $\leq$ 8~s; no timeouts.\\
How test will be performed: Stress-test script measuring T\_ALT\_GEN\_LARGE; record average latency.

\item{UT14-FaultRecovery\\}
Type: Dynamic, Automatic\\
Input/Condition: Force backend process crash.\\
Output/Result: Recovery $\leq$ 5~s; no user data loss.\\
How test will be performed: Docker restart test; verify persistence logs.
\end{enumerate}

\subsubsection{Module 6 \textemdash{} Usability and Accessibility Metrics}

\textbf{Goal:} Quantify user interaction quality and accessibility performance.

\begin{enumerate}
\item{UT15-UsabilitySurvey\\}
Type: Manual, Empirical\\
Input/Condition: Ten participants complete key tasks.\\
Output/Result: Median usability rating $\geq 3$/4; no responses $<2$.\\
How test will be performed: Controlled observation using evaluation rubric.

\item{UT16-ScreenReaderCompatibility\\}
Type: Functional, Dynamic, Manual\\
Input/Condition: Generate alt text and read using NVDA, JAWS, VoiceOver.\\
Output/Result: All screen readers announce output correctly.\\
How test will be performed: Manual auditory confirmation + accessibility log capture.
\end{enumerate}

\subsection{Traceability Between Test Cases and Modules}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Test ID} & \textbf{Module / Feature Tested} & \textbf{Supported Requirement(s)}\\
\hline
UT1--UT3   & Image Upload and Validation          & FR1, PR-RFT1 \\
UT4--UT6   & Alt-Text Generation                  & FR2, FR4, PR-PAR1, PR-PAR2 \\
UT7--UT9   & Accessibility and UI Compliance      & LFR-AR1--AR4, UHR-EUR1--4 \\
UT10--UT12 & Security and Privacy                 & SR-AR1, SR-PR1, PR-SCR1--PR-SCR3 \\
UT13--UT14 & Performance and Reliability          & PR-SL1--2, PR-RFT2 \\
UT15--UT16 & Usability and Accessibility Metrics  & UHR-LR1, UHR-AR1, OER-IAS1 \\
\hline
\end{tabular}
\end{center}
\newpage

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Evaluation Metrics Summary}
\label{appendix:evaluation-metrics}
The following table summarizes the evaluation metrics that will be used to assess the quality and effectiveness of the alternative text generated by the Reading4All system. Each metric includes its scale type, acceptable range, and a brief description of its purpose.
\begin{table}[H]
    \centering
    \caption{Evaluation Metrics Summary}
    \label{tab:evaluation-metrics-summary}
    \begin{tabular}{ |p{3.5cm}|p{3cm}|p{3cm}|p{4cm}| }
      \hline
      \textbf{Metric Name} & \textbf{Scale Type} & \textbf{Acceptable Range} & \textbf{Summary Description} \\
      \hline
      Sufficiency of Description
        & Categorical (1--3)
        & $\geq$ 3 (Sufficient)
        & Does the alt text convey enough information to achieve the intended objective? \\
      \hline
      Length Appropriateness
        & Categorical (1--3)
        & $\geq$ 3 (Proper Length)
        & Is the alt text concise yet complete (not too short or overly verbose)? \\
      \hline
      Accessibility / Usability
        & Numerical (0--3)
        & $\geq$ 2 (Acceptable)
        & Assistive-technology compatibility and clarity; aligns with WCAG 2.1 Level~AA use. \\
      \hline
      Learning Impact
        & Numerical (0--3)
        & $\geq$ 2 (Positive)
        & Does the alt text support or enhance user understanding in learning contexts? \\
      \hline
      Qualitative Feedback Notes
        & Textual
        & N/A
        & Free-form comments on clarity, tone, and suggested improvements. \\
      \hline
    \end{tabular}
  \end{table}

\subsection{SRS Team and Peer Review Checklist}
\label{appendix:srs_checklist}
\textbf{Stakeholders and Users:}
\begin{itemize}
  \item All relevant stakeholders are listed and explained
  \item  Personas clearly explain the stakeholders pain points, needs and their relationship to system being designed. 
\end{itemize}
\textbf{Mandated Constraints:}
\begin{itemize}
  \item All solution constraints are clearly explained, attainable, and measurable. 
\end{itemize}
\textbf{Functional and Non-Functional Requirements:}
\begin{itemize}
  \item Each requirement has a unique identifier.
  \item All core system features have corresponding functional requirement.
  \item Each functional requirement is clearly defined and measurable.
  \item Numerical constraints (ex, number of steps or completion time) are realistic and achievable. 
  \item All the functional requirement are unique, and do not conflict with one another 
  \item All the functional requirements can be traced to a business and product use case.
  \item Each non-functional requirements is clearly defined and measurable. 
  \item All usability and accessibility needs are addressed by a requirement.
  \item Performance related numerical constraints are achievable. 
  \item All the Non-functional requirements are unique, and do not conflict with one another. 
\end{itemize}


\subsection{Verification and Validation Plan Verification Checklist}
\label{appendix:v_v_checklist}

\textbf{General Document Criteria}:
\begin{itemize}
  \item Mission critical qualities are thoroughly discussed and referenced throughout the plan.
  \item Relevant documents such as SRS and HA are referenced and connected to plan. 
\end{itemize}
\textbf{SRS Verification}:
\begin{itemize}
  \item Verification process is thorough and includes key stakeholders.
  \item Provided checklist can guide review process and bring attention to important parts of document. 
  \item Describes a plan for documenting and implementing feedback.
  \item Criteria for evaluating SRS quality is defined. 
  \item  The data collected as evidence for V\&V is clear. 
\end{itemize}
\textbf{Design Verification}:
\begin{itemize}
  \item Design review methods are specified, explained and justified. 
  \item The process for documenting and resolving design review feedback is described. 
  \item  The data collected as evidence for V\&V is clear. 
  \item Automated testing and verification tools are specified. 
\end{itemize}
\textbf{V\&V Plan Verification}:
\begin{itemize}
  \item Verification methods are specified, explained and justified.
  \item Mutation testing will be used to verify the effectiveness of unit tests. 
  \item The data collected as evidence for V\&V is clear. 
\end{itemize}
\textbf{System Tests for Requirements}:
\begin{itemize}
  \item All test cases are detailed and specify input data. 
  \item Survey questions are outlined for usability testing. 
  \item System tests connect and cover all system requirements. 
  \item Traceability between test cases and requirement is clear and documented. 
\end{itemize}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}
\label{appendix:usability}

\begin{enumerate}[label=UA-Q \arabic*., wide=0pt, leftmargin=*]
  \item \emph{Was the length of the generated alternative text description too short, sufficient, or too long?}\\[2mm]
    {\bf Purpose:} This is to gain feedback on whether the length of the text description is sufficient enough 
    for the user to gain understanding on the learning objective of the uploaded image.

  \item \emph{Did the generated alternative text contain too much irrelevant information? If so, how did this affect 
  your ability to learn?}\\[2mm]
    {\bf Purpose:} This is to gain feedback on whether the alternative text contained irrelevant information that deviated 
    from the main learning objective of the uploaded image. It is also to understand what difficulties the stakeholder experiences 
    when there is too much irrelevant information.
  
  \item \emph{Was the generated alternative text contain over-detailed? If so, how did this affect 
  your ability to learn?}\\[2mm]
    {\bf Purpose:} This is to determine whether the generated alternative text includes excessive detail that may hinder user 
    comprehension or distract from the key information needed for effective learning.

  \item \emph{On a scale of 0-4, is the generated alternative text presented in an accessible way? (0 = not accessible at all, 4 = very accessible)}\\[2mm]
    {\bf Purpose:} This is to ensure that the AI-generated alternative text is presented in a manner that aligns with accessibility standards and 
    can be easily perceived, understood, and utilized by users relying on assistive technologies such as screen readers.

  \item \emph{On a scale of 0–4, how easy was it for you to use and understand the generated alternative text? (0 = not easy at all, 4 = very easy)}\\[2mm]
    {\bf Purpose:} This is to assess whether the generated alternative text is presented in a way that supports ease of use and whether it allows users to easily interact with 
    the interface and understand the content with minimal confusion or effort.

  \item \emph{What changes or improvements would you suggest to make this web tool more accessible and enhance your learning experience, if any?}\\[2mm]
  {\bf Purpose:} This question aims to collect additional user feedback on accessibility and usability improvements that may not have been addressed in the previous questions of this survey.

\end{enumerate}


\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable?
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
    successfully complete the verification and validation of your project?
    Examples of possible knowledge and skills include dynamic testing knowledge,
    static testing knowledge, specific tool usage, Valgrind etc.  You
    should look to
    identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
    question, what are at least two approaches to acquiring the knowledge or
    mastering the skill?  Of the identified approaches, which will each team
    member pursue, and why did they make this choice?
\end{enumerate}

\end{document}
