\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{bm}
\usepackage{float}
\usepackage{caption}

\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=black,
  linkcolor=red,
  urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
  \midrule
  Oct 27, 2025 & 1.0 & Initial version of document.\\
  \bottomrule
\end{tabularx}
\newpage
\tableofcontents
\listoftables
\newpage

\section{Symbols, Abbreviations, and Acronyms}
\section*{Symbolic Constants}
\begin{longtable}{|p{8.0cm}|p{8.0cm}|}
  \captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Symbolic Constants used in the \textit{Reading4All} System}
  \label{tab:symbolic-constants} \\
  \toprule
  {\textbf{Name}} & {\textbf{Value}}\\
  \midrule
  T\_ALT\_GEN\_SMALL & 3 seconds(s) \\
  T\_ALT\_GEN\_LARGE & 8 s \\
  T\_UI\_RESP & 300 miliseconds (ms) \\
  R\_SUFFICIENCY & 85\% \\
  R\_LENGTH & 90\% \\
  R\_USABILITY\_MEDIAN & 3 (rating) \\
  R\_USABILITY\_MIN & 2 (rating) \\
  T\_ERROR\_HANDLE & 2 s \\
  T\_RECOVERY & 5 s \\
  CAP\_CONCURRENT & 2 requests \\
  CAP\_STORAGE & 500 images/day \\
  MAINT\_TIME & 2 person-days/quarter \\
  COMPAT\_VERSIONS & 2 releases \\
  IMG\_SIZE\_BIG & 10 MEGABYTES (MB) \\
  IMG\_SIZE\_SMALL & 2 MEGABYTES \\
  TLS\_VERSION & 1.2 \\
  FILE\_DELETE\_TIME & 60 s \\
  FILE\_TYPES & .png, .jpg, .jpeg, .svg, .webp \\
  NETWORK\_SOURCE\_POLICY & McMaster SSO tokens or IP ranges only\\
  TEAM\_SIZE & 5 students \\
  HOURS\_RESEARCH & 40 hours \\
  HOURS\_BACKEND & 120 hours \\
  HOURS\_FRONTEND & 80 hours \\
  HOURS\_TESTING & 60 hours \\
  HOURS\_DOCS & 30 hours \\
  HOURS\_TOTAL & 330 hours \\
  HOURS\_PROJECT & 1,320 person-hours \\
  COST\_PER\_HOUR & \$20/hour(CAD) \qquad\textit{Table continues on next page}\\
  COST\_TOTAL & \$26,400 CAD \\
  COST\_ACTUAL & \$0 CAD \\
  COST\_INCENTIVE\_MIN & \$100 CAD \\
  COST\_INCENTIVE\_MAX & \$150 CAD \\
  MAX\_ZOOM\_PERCENTAGE & 200\% \\
  MIN\_CONTRAST\_RATIO & 4.5:1\\
  MAX\_UPLOAD\_STEPS & 5 steps\\
  MAX\_MINUTES & 5 minutes\\
  USERS\_SUCCESS\_PERCENT & 80\%\\
  MAX\_ERROR\_RECOVER & 2 seconds \\
  LEARNING\_PERCENT & 90\% \\
  MAX\_LEARNING\_MINUTES & 5 minutes \\
  MIN\_COMPENSATION\_DOLLARS & \$100 CAD \\
  MAX\_COMPENSATION\_DOLLARS & \$150 CAD \\
  LATEST\_RELEASES\_NUM & last 3 versions \\
  MOST\_COMMON\_SR & top 3 screen readers \\
  SFWR\_RELEASES & previous 2 releases \\

  \bottomrule
\end{longtable}

\newpage

\pagenumbering{arabic}

\noindent This document outlines the methods Team 22 will take to
ensure the built software meets the intended requirements. To make
certain that the product is built correctly(verification) and that
the right product is built(validation), Team 22 has structured this
document to reflect our plan, by briefly providing our objectives,
and laying out a comprehensive test plan while referring to the
Software Requirements Specification (SRS), Design Document - Module
Guide (MG) and Design Document - Module Interface Specification (MIS). \\

First, Section \ref{geninfo} outlines what software is being tested
with its expected functionality. Here, the objectives and relevant
documentation is mentioned. Next, the plan for verification and
validation is thoroughly explored, touching on the previously
mentioned documents. Finally, each test is mapped to one or more
requirements, ensuring Team 22 delivers what we promised. At the time
of writing this document, unit tests (Section \ref{Unit Test Desc})
was not applicable as it was too early in the project timeline.\\

\section{General Information} \label{geninfo}

\subsection{Summary}

The software being tested is called \textit{Reading4All}. This
software will utilize artificial intelligence (AI) and machine
learning (ML) techniques to provide detailed, context-informed
alternative text for complex technical images, specifically those
found in post-secondary Science, Technology, Engineering and Mathematics (STEM)
course materials. \textit{Reading4All} will allow users to upload
images and then it will automatically produce corresponding
alternative text (alt text), that
meets the described criteria in
Appendix~\ref{appendix:evaluation-metrics} . The system is intended for use by
McMaster University students and faculty, therefore it will include
user validation through McMaster's sign-on, ensuring that only
verified users can access the \textit{Reading4All} system.
In addition, the system will allow users to edit the generated
alternative text, view a history of uploaded images and their
alternative text within that session and download the final outputs
in their desired formats.

\subsection{Objectives}

The objective of this Verification and Validation (VnV) plan is to
build confidence in the correctness, accessibility and usability of
the \textit{Reading4All} system.
The plan focuses on ensuring that the system generates, accurate,
detailed and contextually appropriate alternative text for complex
STEM images, as this is the main functionality of the software.
It also aims to verify that the systems interface is accessible an
usable for individuals with visual impairments, who are one of the
main users of the software. The last object is to demonstrate
effective and accessible usability in secondary features
such as editing the outputted alternative text, viewing session
history and file downloading.
Verifying and validating these objectives is essential to ensure that
users can benefit from the Reading4All system and
it can effectively fulfill its goal of making STEM diagrams more accessible.
\\

Some objectives are out of scope from this VnV plan due to the time
and resource limitations of the project.
The external libraries that might be used in the development of the
system, including PyTorch, TensorFlow, Scikit-Learn, Pandas and
frontend frameworks, will not be verified by our team, as they will
be assumed to have been tested and validated by their implementation teams.
The McMaster sign-on authentication service will also not be tested,
as its maintained and run by the university.

\subsection{Challenge Level and Extras}

The challenge level of the project is set at a \textit{general} level
that will include two additional
components (extras). The first additional component will be a
Norman's Principles report that will
evaluate the design of our final product to ensure that it optimizes
usability and accessibility. The second additional
component will be a user manual that will include comprehensive
documentation to guide users in using the final
product effectively.

\subsection{Relevant Documentation}
The system's Verification and Validation (VnV) plan will reference the
following documents to aid in
the project's assessment and testing:
\begin{enumerate}
  \item \textbf{Software Requirements Specification} (\citet{SRS}):
    This document outlines the key components
    for the VnV plan as it details the functional and non-functional
    requirements of the product. Ensuring that our testing
    satisfies these requirements is essential to meeting the goals of
    the project.
  \item \textbf{Design Document - Module Guide} (\citet{MG}): This
    document outlines how the system is divided into separate module
    and their respective functions.
    This structure helps the VnV Plan by making it easier to test,
    trace, and confirm that each module works correctly and meets the
    requirements.
  \item \textbf{Design Document - Module Interface Specification}
    (\citet{MIS}): This document outlines how the modules of the
    system works and how they interact with each other.
    This aids in our VnV plan as it defines how to validate the
    testing of interactions between the individual parts of the system.
\end{enumerate}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize
this information.}

The V\*V team ensures that all SRS requirements are thorough,
accurately implemented and tested to ensure our system meets them.
Table ~\ref{tab:data-dictionary-reading4all} defines each Reading4All
team members role in confirming that these requirements are well
defined and fulfilled in the system.

\begin{table}[H]
  \centering
  \caption{Verification and Validation Responsibility Breakdown}
  \label{tab:data-dictionary-reading4all}
  \begin{tabular}{ |p{3.0cm}|p{3.8cm}|p{7.3cm}| }
    \hline
    \textbf{Name} & \textbf{ Focus Area } & \textbf{Responsibility} \\
    \hline
    Moly Mikhail and Fiza Sehar & Functional Requirements Tester &
    Completes manuel testing on the software to ensure that
    functional requirements are met. Also oversees any written unit
    tests to ensure they correctly test the system and intended functionality \\
    \hline
    Nawaal Fatima & User Testing Preparation & Coordinates the user
    testing sessions, and oversees the planning and execution of the
    sessions. \\
    \hline
    Casey Francine Bulaclac & Usability Requirements Tester &
    Completes manuel testing to ensure all the defined usability
    requirements are met. Also references WCAG 2.1 crtieria is met,
    prior to automated testing. \\
    \hline
    Dhruv Sardana & Look and Feel Requirements & Evaluates the
    systems design and interface to ensure the outlined requreiments
    are met. Also references WCAG 2.1 criteria is met, prior to
    automated testing. \\
    \hline
    Ms. Jingchuan Sui (Supervisor) & Overall Usability and Quality of
    Alternative Text Reviewer & Reviews and provides feedback to team
    on the overall usability of the system and the quality of the
    generated alternative text. Also helps team complete automating
    evaluation of the system against WCAG 2.1 criteria. \\
    \hline
  \end{tabular}
\end{table}

\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
  read over the SRS.  You should explain your structured approach to the review.
  Will you have a meeting?  What will you present?  What questions will you ask?
  Will you give them instructions for a task-based inspection?  Will
  you use your
issue tracker?}

The verification of the Reading4All SRS will be completed
through a team review, peer review, and supervisor feedback, in that order.
Feedback from each step will be incorporated iteratively to ensure
the document is reviewed and the version presented to our supervisor
reflects the most accurate and complete requirements. \\

\textbf{Team Review}:
As the Reading4All team has already reviewed the outlined
requirements in the SRS document, a team review will
consist of each team member independently evaluating the document
against the checklist provided in Appendix~\ref{appendix:srs_checklist}.   \\

\textbf{Peer Review}:
A peer review will be completed by Team 10 (One of a Kind) to provide
feedback on our SRS document.
They will review the document against the same checklist provided in
Appendix~\ref{appendix:srs_checklist}, ensuring consistency between
how both teams evaluate the document.
This process will allow external reviewers who understand the
documents technical details and software engineering context to
provide feedback on the completeness, understandability and the
overall quality of the requirements. \\

\textbf{Supervisor Feedback}:
We will dedicate one of our weekly meetings with our supervisor to
verifying the SRS document.
During this meeting, will explain each functional and non functional
requirement to ensure every requirement has been documented.
This process will help our supervisor gain an understanding of the
specific requirements we have outlined and keep them in
mind during future validation of our system. \\

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification}
The design verification will be accomplished through the following:
\begin{itemize}
  \item \textbf{Peer Review:} The verification of our design will
    rely on reviews conducted by our fellow classmates who will
    evaluate our design documents including our Module Guide (MG) and
    Module Interface Specification (MIS) using the following
    checklist in Appendix \ref{appendix:dd_checklist}. The peer review will
    be conducted in Week 10 and Week 16 following the due date for
    submission of these design documents.
    Any items on the checklist not satisfied by our team on these
    documents will be addressed through making issues on GitHub.
  \item \textbf{Formal Review:} In a formal review, our supervisor,
    Ms. Jing, will verify that the user interface design supports
    accessibility in accordance with WCAG 2.1 standards.
    Additionally, Capstone teaching assistants and instructors will
    assess the technical aspects of the system, focusing on the
    machine learning architecture to ensure it is well designed.
    The formal review will also be conducted in Week 10 and Week 16
    following the due date for submission of these design documents.
\end{itemize}

\subsection{Verification and Validation Plan Verification}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

The Verification and Validation plan will be verified to ensure its
completeness, and alignment with previous
documents such as our SRS, HA and development plan. This will be
completed through a team and peer review. \\

\textbf{Team Review}\\
The team will perform a review to ensure that the unit testing
described within the document effectively tests the system and
achieves the defined objectives.
This review will involve the team collaboratively going through the
document, identifying areas for improvement and making the necessary
changes before completing further validation activities.\\

\textbf{Peer Review}\\
A peer review will be completed by Team 10 (One of a Kind) to provide
feedback on our V\&V plan. They will review the document to ensure
that the sections are consistent and aligned, as well as to identify
any missing unit tests.

The Verification and Validation Plan Verification for Reading4All
will be completed through a team and peer review, as well as mutation
testing to assess the effectiveness and correctness of our tests. \\

\textbf{Team and Peer Review:}
The \textit{Reading4All} team will complete an internal review to
ensure that the plan is complete and aligns with the SRS
requirements, as well as other documents.
Both our team and peer review will be completed by reviewing the V\&V
document against the checklist provided in
Appendix~\ref{appendix:v_v_checklist}.   \\ \\

\textbf{Mutation Testing:}
Mutation testing will be used to determine whether our unit tests
correctly identify errors within the system.
Small errors will be intentionally be introduced into the prototype
code related to the main functionalities to check whether the test
cases can detect unexpected behavior.

\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
  walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

The implementation verification for Reading4All will utilize the unit
tests defined in section~\ref{Unit Test Desc},
static analyzers and code inspections.

\textbf{Static Analyzers}:
Static analyzers will be used to automatically review any committed
code for errors, and ensure the implementation meets the specified
coding standard.
Tools such as Flake8 and Coverage.py will be integrated into our
development process. Flake8 will identify syntax and style issues,
while Coverage.py can verify that any new code written has unit tests
associated.
Additionally, all unit tests will be executed automatically through
GitHub Actions whenever new code is pushed or a pull request is
opened. This will ensure that recent changes do not introduce any
errors into previously completed features.
Using these static analyzers and workflows will help us to
continuously verify our implementation, ensuring it meets the design
requirements, functional and non functional requirements.

\textbf{Code Inspections}:
As part of our development process, when a feature is completed, team
members are required to open a pull request. The pull request
highlights all the code changes made and is reviewed by other team
members, before merging.
This process ensures that the multiple team members are aware of the
changes being made, and provides an opportunity to give feedback on
the implementation decisions. Additionally it also helps verify that
the code fulfills the intended functionality.

\subsection{Automated Testing and Verification Tools}
The AI Generated Alternative Text tool will make use of the following
automated testing and verification tools:
\begin{itemize}
  \item \textbf{Automated Accessibility Testing}: Automated web
    accessibility testing tools such as the WAVE Web Accessibility
    Evaluation Tool will be used to
    verify that the tool adheres to the Web Content Accessibility
    Guidelines (WCAG) 2.1 guidelines. These tools will automatically
    scan for accessibility issues such as missing alternative text
    and color contrast violations.
  \item \textbf{Unit Testing Framework, Linters, Coverage Tools, and
    Continuous Integration}: These tools have been detailed and
    outlined in the Expected Technologies section
    of our Development Plan (\citet{DP}) document under Table 3.
\end{itemize}

\subsection{Software Validation}
The AI Generated Alternative Text tool will be validated through the following:
\begin{itemize}
  \item \textbf{User Testing}: Stakeholders of this project including
    students experiencing visual
    and cognitive disabilities will perform realistic tasks such as
    uploading multiple images and downloading the alternative text
    to validate that it meets their needs and requirements.
    The team will be present during these sessions to observe and
    note any challenges
    the users face. The participants will also be asked follow-up
    questions highlighted in Appendix \ref{appendix:usability} to
    gain additional feedback and insight
    on their overall experience of using the tool.
  \item \textbf{Formal Reviews}: Reviews will be conducted with the
    stakeholders of the project and our supervisor, Ms. Jing, to
    validate that the requirements
    outlined in our SRS document are fulfilled and met by our tool.
    Additionally, the Rev 0 demo with Ms. Jing will serve as a
    validation review and allow the team
    to receive feedback to improve the tool.
\end{itemize}

\section{System Tests}
\label{System Tests Desc}

This section details tests to cover all requirements as listed in the
Software Requirements Specification (SRS) for Team 22's project. The
tests ensure that the system performs to the predefined standards and
meets user needs.

\subsection{Tests for Functional Requirements}

The tests below cover all six functional requirements as defined in
the SRS document.(I need to add in more summary text here)

\paragraph{Tests for Functional Requirements}

\begin{enumerate}[label=FR-ST \arabic*., wide=0pt, leftmargin=*]
  \item{}
    {\bf Control:} Automatic. \\
    {\bf Initial State:} System is running and ready to accept an
    image upload.\\
    {\bf Input:} Upload files in the following formats:
    \begin{itemize}
      \item Valid: diagram1.jpeg, diagram2.png
      \item Invalid: diagram3.gif, diagram4.pdf
    \end{itemize}
    {\bf Output:} The system accepts .jpeg and .png images and
    displays an error message (e.g., “Invalid file type”) for .gif
    and .pdf uploads.\\
    {\bf  Case Derivation:}According to the \textbf{FR 1} criterion, the system
    must accept JPEG and PNG formats and reject all others with proper
    feedback. Therefore, valid formats are processed, and invalid
    formats trigger an error.\\
    {\bf How test will be performed:} The test can be performed by
    uploading a sample set of image files in different formats (JPEG,
    PNG, GIF, PDF) to the system. The system’s responses will be
    observed to confirm that only JPEG and PNG files are accepted,
    while others trigger an appropriate error message.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:} System running with image upload
    functionality active.\\
    {\bf Input:} Upload a set of test diagrams (diagram1.png, diagram2.jpeg).\\
    {\bf Output:} The system generates alternative text descriptions
    for each uploaded image that meet pre-determined quality or
    clarity criteria (e.g., contains key diagram elements, concise
    description, no missing components).\\
    {\bf  Case Derivation:}As specified by \textbf{FR 2} criterion,
    the system must correctly process JPEG and PNG files while
    rejecting all other formats. Therefore, the expected outcome is
    that valid images are accepted without error, and invalid formats
    trigger a clear feedback message to the user.\\
    {\bf How test will be performed:} The test can be performed by
    uploading a test set of sample diagrams to the system and
    reviewing the generated alternative text. The generated text will
    be compared against predetermined quality criteria or expected
    reference outputs to verify accuracy and completeness.\\

  \item{}
    {\bf Control:} Manual\\
    {\bf Initial State:} Alternative text has been generated for at
    least one uploaded image.\\
    {\bf Input:} Use screen readers such as NVDA, JAWS, and VoiceOver
    to read the outputted alternative text.\\
    {\bf Output:} Alternative text is fully read aloud by at least
    the most common screen readers without truncation, misreading, or
    formatting errors. \\
    {\bf  Case Derivation:} Given the \textbf{FR 3} criterion for
    compatibility with commonly used screen readers, the expected
    outcome is that the generated alternative text will be fully
    readable and correctly interpreted by tools such as NVDA, JAWS,
    and VoiceOver without truncation or mispronunciation.\\
    {\bf How test will be performed:} The test can be performed by
    enabling common screen readers such as NVDA, JAWS, and VoiceOver
    to read the generated alternative text aloud. Observations will
    confirm whether the text is read fully, clearly, and without
    formatting or accessibility issues.\\

  \item{}
    {\bf Control:} Manual\\
    {\bf Initial State:} Generated alternative text is visible to the user.\\
    {\bf Input:}User edits the outputted text (adds words, deletes
    sentences, modifies phrasing) and saves changes. \\
    {\bf Output:} The system reflects the user’s edits accurately and
    stores the updated version without loss of data or formatting errors.\\
    {\bf  Case Derivation:} As stated in \textbf{FR 4} criterion,
    users must be able to modify any part of the generated
    alternative text and save their changes. The expected result is
    that all edits are accurately captured, stored, and displayed
    without data loss or formatting issues.\\
    {\bf How test will be performed:} The test can be performed by
    selecting the generated alternative text and performing a series
    of edits—adding, deleting, and modifying words—then saving the
    changes. The output will be reviewed to ensure that edits are
    accurately reflected and retained.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:}User logged in and has uploaded at least one
    image with generated alt text.\\
    {\bf Input:} Upload multiple images sequentially within the same
    session, then navigate through the session interface.\\
    {\bf Output:} All previously uploaded images and their
    corresponding alt texts remain visible and accessible until the
    user logs out or the session ends.\\
    {\bf  Case Derivation:}According to \textbf{FR 5} criterion, all
    uploaded images and their corresponding alternative texts should
    remain visible within the same session. Therefore, the expected
    outcome is that users can access and review all prior uploads
    without reloading or re-uploading them during an active session.\\
    {\bf How test will be performed:} The test can be performed by
    uploading multiple images within the same session, then
    navigating across different pages or refreshing the interface.
    The test will verify that all uploaded images and their
    corresponding alternative texts remain visible until the session ends.\\

  \item{}
    {\bf Control:} Automatic\\
    {\bf Initial State:} Login page displayed.\\
    {\bf Input:} Access for login is defined below:
    \begin{itemize}
      \item Valid credentials: McMaster University email and password
      \item Invalid credentials: non-McMaster email or incorrect password
    \end{itemize}
    {\bf Output:} Access granted only to users with valid McMaster
    credentials. Invalid attempts are rejected with an appropriate
    error message (e.g., “Invalid login credentials”).\\
    {\bf  Case Derivation:}As outlined in the \textbf{FR 6}
    criterion, only users with verified McMaster University
    credentials should gain access to system features. The expected
    outcome is that valid users can log in successfully, while
    unauthorized or invalid attempts are denied with an appropriate
    error message.\\
    {\bf How test will be performed:} The test can be performed by
    attempting to log in using both valid McMaster University
    credentials and invalid credentials. The system’s behavior will
    be reviewed to confirm that only verified users gain access,
    while invalid attempts produce an appropriate error message.\\
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

{Need a small summary blurb here pls. - NF}

\paragraph{Tests for Non-Functional Requirements}

\begin{enumerate}[label=NFR-ST \arabic*., wide=0pt, leftmargin=*]

  \item{}
    \textbf{Text Resizing and Contrast Accessibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} LFR-AR1, LFR-AR2, LFR-AR3, LFR-AR4. \\
    \textbf{Initial State:} Interface displayed in a standard browser
    with accessibility tools enabled. \\
    \textbf{Input/Condition:} The user adjusts browser zoom to the
    maximum allowed and reviews color usage. \\
    \textbf{Output/Result:} Text resizes correctly without overlap;
    information is not conveyed by color alone; contrast meets
    accessibility thresholds; all images have alternative text. \\[2mm]
    \textbf{How test will be performed:} The tester will manually
    adjust zoom levels, use color contrast tools, and run screen
    reader tests to confirm accessibility compliance.

  \item \textbf{Interface Style and Branding Verification}. \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} LFR-SR1, LFR-SR2, LFR-SR3. \\
    \textbf{Initial State:} System interface displayed in a browser. \\
    \textbf{Input/Condition:} Visual inspection of layout, font,
    colors, branding, and adherence to Norman's design principles. \\
    \textbf{Output/Result:} Interface maintains modern and simple
    style, McMaster branding is present without interfering with
    usability, and design elements comply with Norman’s principles. \\[2mm]
    \textbf{How test will be performed:} Tester will review the
    interface against the style guide, branding requirements, and
    Norman’s design checklist to ensure compliance.

  \item \textbf{Usability Efficiency and Learnability}. \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic. \\
    \textbf{Covers:} UHR-EUR1, UHR-EUR2, UHR-EUR3, UHR-EUR4 \\
    \textbf{Initial State:} System interface available to first-time
    and returning users \\
    \textbf{Input/Condition:} Users perform key actions: login,
    upload images, generate alt text \\
    \textbf{Output/Result:} Users complete tasks efficiently, recall
    steps after a break, receive feedback within 1 second, and can
    correct errors easily \\[2mm]
    \textbf{How test will be performed:} Conduct usability sessions
    with participants performing all major tasks while timing
    actions, recording feedback response, and monitoring error recovery.

  \item \textbf{Alt Text Storage and Personalization Options} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-PIR1 \\
    \textbf{Initial State:} Alt text generated for uploaded image \\
    \textbf{Input/Condition:} User chooses to copy or download
    generated alt text \\
    \textbf{Output/Result:} Alt text is successfully copied to
    clipboard or downloaded as .txt \\[2mm]
    \textbf{How test will be performed:} Tester generates alt text
    and selects each option, confirming that the system executes the
    chosen storage method correctly.

  \item \textbf{Accessibility for Screen Readers and Low-Vision Users} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} UHR-LR1, UHR-AR1, UHR-AR2\\
    \textbf{Initial State:} Interface accessible with popular screen readers \\
    \textbf{Input/Condition:} Users with screen readers upload images
    and generate alt text \\
    \textbf{Output/Result:} Users successfully generate alt text
    within the time limit and can navigate interface efficiently \\[2mm]
    \textbf{How test will be performed:} Conduct sessions with
    low-vision participants using NVDA, JAWS, or VoiceOver and
    measure task completion time and success rates.

  \item \textbf{Performance – Alt Text Generation Time} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-SL1, PR-SL2 \\
    \textbf{Initial State:} System under typical load conditions \\
    \textbf{Input/Condition:} Upload images of various sizes (small
    and large) \\
    \textbf{Output/Result:} Generated alt text returned within
    specified thresholds for each image size; UI responds within
    T\_UI\_RESP \\[2mm]
    \textbf{How test will be performed:} Automated scripts upload
    images and record generation time and UI response time; results
    plotted to verify performance meets criteria.

  \item \textbf{Safety and Timeout Handling} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-SR-HA1, PR-SR-HA2, PR-SR-HA3 \\
    \textbf{Initial State:} System ready for alt text generation \\
    \textbf{Input/Condition:} Simulate long-running or stalled image
    processing \\
    \textbf{Output/Result:} User notified of timeout, option to
    retry; incomplete data is deleted; messages do not reveal
    technical details \\ [2mm]
    \textbf{How test will be performed:} Tester simulates timeouts
    and verifies notifications, data cleanup, and absence of
    technical information in messages.

  \item \textbf{Alt Text Accuracy and Usability Evaluation} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-PAR1, PR-PAR2, PR-PAR3, CR-LR1 \\
    \textbf{Initial State:} Alt text generated for uploaded images \\
    \textbf{Input/Condition:} Users rate generated alt text for
    sufficiency, length, readability, and usability \\
    \textbf{Output/Result:} Ratings meet R\_SUFFICIENCY, R\_LENGTH, and
    R\_USABILITY\_MEDIAN thresholds \\[2mm]
    \textbf{How test will be performed:} Conduct structured user
    evaluation using rating scales; calculate statistics to confirm
    compliance with quality thresholds.

  \item \textbf{Robustness to Invalid Inputs and Fault Recovery} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} PR-RFT1, PR-RFT2 \\
    \textbf{Initial State:} System running normally \\
    \textbf{Input/Condition:} Upload unsupported, corrupted files, or
    simulate backend process failures \\
    \textbf{Output/Result:} Clear error messages displayed; system
    recovers within T\_RECOVERY \\[2mm]
    \textbf{How test will be performed:} Tester uploads invalid files
    and observes error handling; automated test simulates isolated
    failures to confirm automatic recovery.

  \item \textbf{Concurrent Usage and Storage Capacity} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-CR1, PR-CR2 \\
    \textbf{Initial State:} System deployed in test environment \\
    \textbf{Input/Condition:} Simulate multiple simultaneous users
    and upload datasets \\
    \textbf{Output/Result:} Supports CAP\_CONCURRENT users with
    response times $\le$ 10s; storage handles CAP\_STORAGE datasets \\[2mm]
    \textbf{How test will be performed:} Load testing scripts
    simulate multiple concurrent requests and dataset uploads;
    performance and storage usage monitored.

  \item \textbf{System Extensibility and Maintainability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} PR-SER1, PR-LR1, PR-LR2, MS-MNT1, MS-MNT2,
    MS-MNT3, MS-AD1, MS-AD2, MS-AD3 \\
    \textbf{Initial State:} Existing modular system codebase deployed \\
    \textbf{Input/Condition:} Apply updates to modules,
    configuration, or AI models \\
    \textbf{Output/Result:} System maintains functionality; new
    modules integrate without breaking existing components; changes
    tracked \\[2mm]
    \textbf{How test will be performed:} Tester modifies components
    and configuration files, runs automated CI/CD tests, and verifies
    integration of new modules.

  \item \textbf{Security, Access, and Network Restrictions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual/Automated, Dynamic \\
    \textbf{Covers:} SR-AR1, SR-AR2, SR-IR1, SR-IR2, SR-PR1, SR-PR2,
    SR-AU1, SR-AU2, SR-IM1, SR-IM2 \\
    \textbf{Initial State:} System deployed with SSO and HTTPS enabled \\
    \textbf{Input/Condition:} Attempt unauthorized access, upload
    images, and inspect logs \\
    \textbf{Output/Result:} Only authorized McMaster users gain
    access; encrypted communication enforced; uploaded images
    deleted; PII filtered; logs access restricted; unsupported files
    rejected; external networks blocked \\[2mm]
    \textbf{How test will be performed:} Testers attempt invalid
    logins, inspect encrypted traffic, upload images and check
    deletion, validate moderation filters, and verify network
    restrictions and audit log access.

  \item \textbf{Cultural and Professional Content Compliance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} CR1, CR2, CR3 \\
    \textbf{Initial State:} Alt text generated by the system \\
    \textbf{Input/Condition:} Review generated alt text samples \\
    \textbf{Output/Result:} Text is neutral, inclusive, contextually
    accurate, and professional \\[2mm]
    \textbf{How test will be performed:} Tester inspects a variety of
    outputs for bias, unnecessary cultural references, and tone appropriateness.

  \item \textbf{Compliance and Regulatory Verification} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} CR-LR1, CR-SCR1, CR-SCR2 \\
    \textbf{Initial State:} System generates alt text and manages
    uploaded data \\
    \textbf{Input/Condition:} Validate against AODA, WCAG 2.1, and
    institutional privacy policies \\
    \textbf{Output/Result:} Generated alt text meets accessibility
    standards; uploaded files handled per policy; documentation
    available for stakeholders \\[2mm]
    \textbf{How test will be performed:} Run accessibility and
    privacy compliance tests; inspect system logs and documentation
    for adherence.

  \item \textbf{Environmental and Device Compatibility} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} OER-EP1, OER-EP2, OER-WE1, OER-WE2 \\
    \textbf{Initial State:} System installed on multiple devices and
    OS platforms \\
    \textbf{Input/Condition:} Access system in standard classroom,
    office, or cloud environments \\
    \textbf{Output/Result:} System functions reliably across devices,
    platforms, and typical indoor conditions; maintains network
    connectivity \\[2mm]
    \textbf{How test will be performed:} Testers access the system on
    Windows, Mac, and Linux with various browsers; verify full functionality.

  \item \textbf{Image Alt Text Accuracy Metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-VAL1, PR-VAL2 \\
    \textbf{Initial State:} Alt text generation operational \\
    \textbf{Input/Condition:} Process test image set \\
    \textbf{Output/Result:} Accuracy metrics calculated; performance
    within target thresholds \\[2mm]
    \textbf{How test will be performed:} Automated evaluation against
    ground truth alt text; calculate accuracy, precision, and recall.

  \item \textbf{Privacy of Uploaded Images} \\[2mm]
    \textbf{Type:} Safety-Critical, Manual and Automated, Dynamic \\
    \textbf{Covers:} PR-SCR1 \\
    \textbf{Initial State:} User session active; image upload
    interface loaded \\
    \textbf{Input/Condition:} Upload test images without opting to save \\
    \textbf{Output/Result:} Uploaded images removed from temporary
    storage; no images stored in database \\[2mm]
    \textbf{How test will be performed:}
    Upload images, complete user session (logout or timeout), inspect
    temporary storage and databases to verify images are deleted and
    no personally identifiable data remains.

  \item \textbf{Offensive or Biased Alt Text Prevention} \\[2mm]
    \textbf{Type:} Safety-Critical, Automated, Dynamic \\
    \textbf{Covers:} PR-SCR2 \\
    \textbf{Initial State:} Alt text generation operational \\
    \textbf{Input/Condition:} Process diverse test image set,
    including sensitive content \\
    \textbf{Output/Result:} Generated alt text contains no offensive,
    biased, or harmful language \\[2mm]
    \textbf{How test will be performed:}
    Generate alt text for each test image, pass outputs through
    moderation filters, confirm 0 percentage flagged content.

  \item \textbf{WCAG 2.1 Level AA Accessibility Compliance} \\[2mm]
    \textbf{Type:} Safety-Critical, Manual and Automated, Dynamic \\
    \textbf{Covers:} PR-SCR3 \\
    \textbf{Initial State:} Interface loaded and interactive \\
    \textbf{Input/Condition:} Navigate and interact with all
    interface elements \\
    \textbf{Output/Result:} Interface meets WCAG 2.1 Level AA
    criteria; no visual strain or accessibility barriers \\[2mm]
    \textbf{How test will be performed:}
    Perform automated accessibility scans (e.g., axe, Lighthouse) and
    manual checks for color contrast, text resizing, keyboard
    navigation, and visual comfort to confirm compliance.

  \item \textbf{Screen Reader Interoperability} \\[2mm]
    \textbf{Type:} Functional, Manual and Automated, Dynamic \\
    \textbf{Covers:} OER-IAS1 \\
    \textbf{Initial State:} Alt text generation operational \\
    \textbf{Input/Condition:} Generate alt text and access it using
    major screen readers (NVDA, JAWS, VoiceOver) \\
    \textbf{Output/Result:} Alt text is correctly parsed and read
    aloud without formatting issues \\[2mm]
    \textbf{How test will be performed:}
    Feed generated alt text to NVDA, JAWS, and VoiceOver; verify
    correct pronunciation, formatting, and comprehension; record any
    errors or misread content.

  \item \textbf{Supported Image Format Processing} \\[2mm]
    \textbf{Type:} Functional, Automated, Dynamic \\
    \textbf{Covers:} OER-IAS2 \\
    \textbf{Initial State:} Image upload interface operational \\
    \textbf{Input/Condition:} Upload images in JPG, JPEG, and PNG formats \\
    \textbf{Output/Result:} System correctly processes images and
    generates accurate alt text \\[2mm]
    \textbf{How test will be performed:}
    Prepare a set of test images in each supported format, upload to
    the system, generate alt text, and validate output accuracy
    against expected descriptions.

  \item \textbf{Automated Accessibility Validator Integration} \\[2mm]
    \textbf{Type:} Functional, Manual and Automated, Dynamic \\
    \textbf{Covers:} OER-IAS3 \\
    \textbf{Initial State:} System operational with validator
    interface enabled \\
    \textbf{Input/Condition:} Trigger accessibility validation using
    WAVE or Axe \\
    \textbf{Output/Result:} Validation reports are successfully
    generated and accessible through the interface \\[2mm]
    \textbf{How test will be performed:}
    Run alt text through integrated validation tools; confirm report
    generation, correct display in UI, and accurate reflection of
    accessibility issues.

  \item \textbf{Web Tool Deployment} \\[2mm]
    \textbf{Type:} Productization, Manual and Automated, Dynamic \\
    \textbf{Covers:} OER-PR1 \\
    \textbf{Initial State:} System hosted and deployed on test server \\
    \textbf{Input/Condition:} Access web tool from multiple
    institutional environments \\
    \textbf{Output/Result:} Tool is accessible, functional, and
    validated through institutional testing \\[2mm]
    \textbf{How test will be performed:}
    Deploy the system on institutional server; verify accessibility,
    authentication, alt text generation, and performance; document
    compliance with institutional standards.

  \item \textbf{Core Feature Verification Before Release} \\[2mm]
    \textbf{Type:} Release, Manual and Automated, Dynamic \\
    \textbf{Covers:} OER-RL1 \\
    \textbf{Initial State:} System fully implemented in test environment \\
    \textbf{Input/Condition:} Execute image analysis, text
    generation, and accessibility validation modules \\
    \textbf{Output/Result:} All core features function correctly;
    verification and validation documentation confirms compliance \\[2mm]
    \textbf{How test will be performed:}
    Run full test suite for each core feature; document results,
    record any failures, and confirm all functional requirements are
    met prior to release.

  \item \textbf{Release Readiness for Capstone Demonstration} \\[2mm]
    \textbf{Type:} Release, Manual and Automated, Dynamic \\
    \textbf{Covers:} OER-RL2 \\
    \textbf{Initial State:} System deployed to staging environment \\
    \textbf{Input/Condition:} Conduct full system walkthrough aligned
    with March 2026 Capstone schedule \\
    \textbf{Output/Result:} System fully functional, accessible, and
    ready for demonstration \\[2mm]
    \textbf{How test will be performed:}
    Perform end-to-end functional testing, accessibility
    verification, and deployment validation; ensure all components
    operate as expected and system is stable for final presentation.

  \item \textbf{Performance and Metrics Logging} \\[2mm]
    \textbf{Type:} Maintenance Support, Automated, Dynamic \\
    \textbf{Covers:} MS-SUP1 \\
    \textbf{Initial State:} System operational with
    monitoring/logging enabled \\
    \textbf{Input/Condition:} Execute standard workflows including
    image uploads, alt text generation, and API interactions \\
    \textbf{Output/Result:} Metrics for API calls, latency, error
    rates, and model confidence scores are logged and accessible \\[2mm]
    \textbf{How test will be performed:}
    Perform typical user operations; monitor logs and dashboards;
    export reports to verify all key metrics are accurately recorded,
    securely stored, and available for analysis.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{longtable}{|p{8.0cm}|p{8.0cm}|}
  \captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Connections between \textit{Reading4All} system tests and
  SRS requirements}
  \label{tab:sys-tests} \\
  \toprule
  {\textbf{Test ID}} & {\textbf{Requirement ID(as per SRS)}}\\
  \midrule
  FR-ST 1 & FR-1 \\ \hline
  FR-ST 2 & FR-2 \\ \hline
  FR-ST 3 & FR-3 \\ \hline
  FR-ST 4 & FR-4 \\ \hline
  FR-ST 5 & FR-5 \\ \hline
  FR-ST 6 & FR-6 \\ \hline
  NFR-ST 1 & LFR-AR1, LFR-AR2, LFR-AR3, LFR-AR4. \\ \hline
  NFR-ST 2 & LFR-SR1, LFR-SR2, LFR-SR3 \\ \hline
  NFR-ST 3 & UHR-EUR1, UHR-EUR2, UHR-EUR3, UHR-EUR4 \\ \hline
  NFR-ST 4 & UHR-PIR1 \\ \hline
  NFR-ST 5 & UHR-LR1, UHR-AR1, UHR-AR2\\ \hline
  NFR-ST 6 & PR-SL1, PR-SL2\\ \hline
  NFR-ST 7 & PR-SR-HA1, PR-SR-HA2, PR-SR-HA3 \\ \hline
  NFR-ST 8 & PR-PAR1, PR-PAR2, PR-PAR3, CR-LR1 \\ \hline
  NFR-ST 9 & PR-RFT1, PR-RFT2 \\ \hline
  NFR-ST 10 & PR-CR1, PR-CR2 \\ \hline
  NFR-ST 11 & PR-SER1, PR-LR1, PR-LR2, MS-MNT1, MS-MNT2,MS-MNT3, MS-AD1, MS-AD2, MS-AD3 \\ \hline
  NFR-ST 12 & SR-AR1, SR-AR2, SR-IR1, SR-IR2, SR-PR1, SR-PR2, SR-AU1, SR-AU2, SR-IM1, SR-IM2 \\ \hline
  NFR-ST 13 & CR1, CR2, CR3 \\ \hline
  NFR-ST 14 & CR-LR1, CR-SCR1, CR-SCR2 \\ \hline
  NFR-ST 15 & OER-EP1, OER-EP2, OER-WE1, OER-WE2 \\ \hline
  NFR-ST 16 & PR-VAL1, PR-VAL2 \\ \hline
  NFR-ST 17 & PR-SCR1 \\ \hline
  NFR-ST 18 & PR-SCR2 \\ \hline
  NFR-ST 19 & PR-SCR3 \\ \hline
  NFR-ST 20 & OER-IAS1 \\ \hline
  NFR-ST 21 & OER-IAS2 \\ \hline
  NFR-ST 22 & OER-IAS3 \\ \hline
  NFR-ST 23 & OER-PR1 \\ \hline
  NFR-ST 24 & OER-RL1 \\ \hline
  NFR-ST 25 & OER-RL2 \\ \hline
  NFR-ST 26 & MS-SUP1 \\
  \bottomrule
\end{longtable}

\section{Unit Test Description}
\label{Unit Test Desc}

\subsection{Unit Testing Scope}

The purpose of unit testing for \textit{Reading4All} is to verify the
correctness, robustness, and accessibility compliance of individual
components prior to system integration. Each module is tested
independently using automated test scripts (PyTest) and deterministic
input fixtures.

\textbf{Modules in Scope:}
\begin{itemize}
  \item Image Upload \& Validation Module
  \item Alt-Text Generation Module
  \item Accessibility and UI Compliance Module
  \item Security and Privacy Module
\end{itemize}

\textbf{Modules Out of Scope:}
Third-party OCR engines, pretrained vision/language models, and
external McMaster authentication services are assumed to be validated
independently. Only the thin wrappers and internal interactions with
these APIs are tested here.

\subsection{Tests for Functional Requirements}

\subsubsection{Module 1 \textemdash{} Image Upload and Validation}

\textbf{Goal:} Ensure uploaded images meet all input constraints for
format, size, and type, and that invalid files are rejected gracefully.

\begin{enumerate}
  \item{UT1-UploadValidImage\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: Application running; no active uploads.\\
    Input: Valid PNG image, 1 MB in size.\\
    Output: File accepted; confirmation message displayed; metadata
    stored in temporary session.\\
    Test Case Derivation: Confirms compliance with input constraints
    (JPEG/PNG $\leq$ 10 MB).\\
    How test will be performed: Run automated pytest verifying HTTP 200
    response and valid JSON schema.

  \item{UT2-UploadInvalidFileType\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: No uploads.\\
    Input: Unsupported file type (e.g., \texttt{.pdf}).\\
    Output: Error message ``Unsupported file format'' returned; no
    data stored.\\
    Test Case Derivation: Validates enforcement of file type constraint
    and secure rejection.\\
    How test will be performed: Send POST request with invalid MIME
    type; assert error 400 and log entry.

  \item{UT3-UploadOversizedFile\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: No uploads.\\
    Input: PNG file $>$ 10 MB.\\
    Output: Upload rejected with clear error; no file stored.\\
    Test Case Derivation: Confirms handling of maximum size threshold.\\
    How test will be performed: Simulate multipart upload; verify
    memory cleanup and error alert.
\end{enumerate}

\subsubsection{Module 2 \textemdash{} Alt-Text Generation}

\textbf{Goal:} Validate that image inference and text generation
components produce deterministic, relevant, and correctly formatted
alternative text.

\begin{enumerate}
  \item{UT4-GenerateAltText\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: Valid image uploaded and accessible to model service.\\
    Input: Image containing labeled diagram.\\
    Output: Non-empty descriptive string within 3--8~s latency window.\\
    Test Case Derivation: Confirms compliance with generation timing
    and content sufficiency.\\
    How test will be performed: Mock ML service; assert response schema
    and timing $<$ T\_ALT\_GEN\_SMALL.

  \item{UT5-EditAltText\\}
    Type: Functional, Dynamic, Manual\\
    Initial State: Alt-text successfully generated.\\
    Input: User edits description and saves.\\
    Output: Edited text replaces old version in session storage.\\
    Test Case Derivation: Ensures edit functionality modifies session
    data only.\\
    How test will be performed: Selenium automation of UI; assert saved
    value persists on reload.

  \item{UT6-HandleEmptyAltText\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: Image uploaded yields no model output.\\
    Input: Blank model return.\\
    Output: Error message and retry option; no text stored.\\
    Test Case Derivation: Confirms graceful failure and user notification.\\
    How test will be performed: Patch model API to return empty string;
    validate error log.
\end{enumerate}

\subsubsection{Module 3 \textemdash{} Accessibility and UI Compliance}

\textbf{Goal:} Validate that all UI components meet accessibility and
usability criteria (keyboard navigation, zoom, color contrast, alt
text labeling).

\begin{enumerate}
  \item{UT7-KeyboardNavigation\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: Application home screen loaded.\\
    Input: Simulated Tab and Enter key presses.\\
    Output: All focusable elements reachable; no trap detected.\\
    Test Case Derivation: Confirms WCAG 2.1 Success Criterion 2.1.1.\\
    How test will be performed: Automated Axe/WAVE accessibility scan
    with keyboard simulation.

  \item{UT8-ContrastValidation\\}
    Type: Functional, Static, Automatic\\
    Initial State: Deployed UI snapshot available.\\
    Input: CSS stylesheet.\\
    Output: All color pairs $\geq$ 4.5:1 contrast ratio.\\
    Test Case Derivation: Confirms MIN\_CONTRAST\_RATIO threshold met.\\
    How test will be performed: Run Lighthouse CI contrast-check script.

  \item{UT9-ZoomResilience\\}
    Type: Functional, Manual\\
    Initial State: Browser window at 100\%.\\
    Input: Zoom increased to 200\%.\\
    Output: Interface remains fully visible and interactive.\\
    Test Case Derivation: Ensures compliance with MAX\_ZOOM\_PERCENTAGE.\\
    How test will be performed: Manual inspection + screen-reader pass.
\end{enumerate}

\subsubsection{Module 4 \textemdash{} Security and Privacy}

\textbf{Goal:} Verify that all authentication, encryption, and
data-deletion procedures uphold confidentiality and integrity requirements.

\begin{enumerate}
  \item{UT10-LoginAuthentication\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: No user session.\\
    Input: Valid McMaster SSO credentials.\\
    Output: Access granted; session token stored.\\
    Test Case Derivation: Ensures access restriction and session linking.\\
    How test will be performed: Mock OAuth SSO; assert 200 and JWT valid.

  \item{UT11-RejectUnauthorizedAccess\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: No valid session token.\\
    Input: API request to /generate endpoint.\\
    Output: HTTP 401 Unauthorized.\\
    Test Case Derivation: Confirms secure access control.\\
    How test will be performed: Post request without auth header;
    verify denial and log entry.

  \item{UT12-TemporaryFileDeletion\\}
    Type: Functional, Dynamic, Automatic\\
    Initial State: Completed alt-text generation.\\
    Input: Wait $>$ 60~s.\\
    Output: Uploaded file deleted from temporary directory.\\
    Test Case Derivation: Verifies privacy compliance (FILE\_DELETE\_TIME).\\
    How test will be performed: Check directory contents before/after timeout.
\end{enumerate}

\subsection{Tests for Non-Functional Requirements}

\subsubsection{Module 5 \textemdash{} Performance and Reliability}

\textbf{Goal:} Ensure responsiveness, stability, and error handling
meet defined thresholds.

\begin{enumerate}
  \item{UT13-LatencyBenchmark\\}
    Type: Dynamic, Automatic\\
    Input/Condition: Upload 5 images $\leq$ 2~MB each concurrently.\\
    Output/Result: Mean response $\leq$ 8~s; no timeouts.\\
    How test will be performed: Stress-test script measuring
    T\_ALT\_GEN\_LARGE; record average latency.

  \item{UT14-FaultRecovery\\}
    Type: Dynamic, Automatic\\
    Input/Condition: Force backend process crash.\\
    Output/Result: Recovery $\leq$ 5~s; no user data loss.\\
    How test will be performed: Docker restart test; verify persistence logs.
\end{enumerate}

\subsubsection{Module 6 \textemdash{} Usability and Accessibility Metrics}

\textbf{Goal:} Quantify user interaction quality and accessibility performance.

\begin{enumerate}
  \item{UT15-UsabilitySurvey\\}
    Type: Manual, Empirical\\
    Input/Condition: Ten participants complete key tasks.\\
    Output/Result: Median usability rating $\geq 3$/4; no responses $<2$.\\
    How test will be performed: Controlled observation using evaluation rubric.

  \item{UT16-ScreenReaderCompatibility\\}
    Type: Functional, Dynamic, Manual\\
    Input/Condition: Generate alt text and read using NVDA, JAWS, VoiceOver.\\
    Output/Result: All screen readers announce output correctly.\\
    How test will be performed: Manual auditory confirmation +
    accessibility log capture.
\end{enumerate}

\subsection{Traceability Between Test Cases and Modules}

\begin{center}
  \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Test ID} & \textbf{Module / Feature Tested} &
    \textbf{Supported Requirement(s)}\\
    \hline
    UT1--UT3   & Image Upload and Validation          & FR1, PR-RFT1 \\
    UT4--UT6   & Alt-Text Generation                  & FR2, FR4,
    PR-PAR1, PR-PAR2 \\
    UT7--UT9   & Accessibility and UI Compliance      & LFR-AR1--AR4,
    UHR-EUR1--4 \\
    UT10--UT12 & Security and Privacy                 & SR-AR1, SR-PR1,
    PR-SCR1--PR-SCR3 \\
    UT13--UT14 & Performance and Reliability          & PR-SL1--2, PR-RFT2 \\
    UT15--UT16 & Usability and Accessibility Metrics  & UHR-LR1,
    UHR-AR1, OER-IAS1 \\
    \hline
  \end{tabular}
\end{center}
\newpage

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Evaluation Metrics Summary}
\label{appendix:evaluation-metrics}
The following table summarizes the evaluation metrics that will be
used to assess the quality and effectiveness of the alternative text
generated by the Reading4All system. Each metric includes its scale
type, acceptable range, and a brief description of its purpose.
\begin{table}[H]
  \centering
  \caption{Evaluation Metrics Summary}
  \label{tab:evaluation-metrics-summary}
  \begin{tabular}{ |p{3.5cm}|p{3cm}|p{3cm}|p{4cm}| }
    \hline
    \textbf{Metric Name} & \textbf{Scale Type} & \textbf{Acceptable
    Range} & \textbf{Summary Description} \\
    \hline
    Sufficiency of Description
    & Categorical (1--3)
    & $\geq$ 3 (Sufficient)
    & Does the alt text convey enough information to achieve the
    intended objective? \\
    \hline
    Length Appropriateness
    & Categorical (1--3)
    & $\geq$ 3 (Proper Length)
    & Is the alt text concise yet complete (not too short or overly verbose)? \\
    \hline
    Accessibility / Usability
    & Numerical (0--3)
    & $\geq$ 2 (Acceptable)
    & Assistive-technology compatibility and clarity; aligns with
    WCAG 2.1 Level~AA use. \\
    \hline
    Learning Impact
    & Numerical (0--3)
    & $\geq$ 2 (Positive)
    & Does the alt text support or enhance user understanding in
    learning contexts? \\
    \hline
    Qualitative Feedback Notes
    & Textual
    & N/A
    & Free-form comments on clarity, tone, and suggested improvements. \\
    \hline
  \end{tabular}
\end{table}

\subsection{SRS Team and Peer Review Checklist}
\label{appendix:srs_checklist}
\textbf{Stakeholders and Users:}
\begin{itemize}
  \item All relevant stakeholders are listed and explained
  \item  Personas clearly explain the stakeholders pain points, needs
    and their relationship to system being designed.
\end{itemize}
\textbf{Mandated Constraints:}
\begin{itemize}
  \item All solution constraints are clearly explained, attainable, and
    measurable.
\end{itemize}
\textbf{Functional and Non-Functional Requirements:}
\begin{itemize}
  \item Each requirement has a unique identifier.
  \item All core system features have corresponding functional requirement.
  \item Each functional requirement is clearly defined and measurable.
  \item Numerical constraints (ex, number of steps or completion time)
    are realistic and achievable.
  \item All the functional requirement are unique, and do not conflict
    with one another
  \item All the functional requirements can be traced to a business and
    product use case.
  \item Each non-functional requirements is clearly defined and measurable.
  \item All usability and accessibility needs are addressed by a requirement.
  \item Performance related numerical constraints are achievable.
  \item All the Non-functional requirements are unique, and do not
    conflict with one another.
\end{itemize}

\subsection{Verification and Validation Plan Verification Checklist}
\label{appendix:v_v_checklist}

\textbf{General Document Criteria}:
\begin{itemize}
  \item Mission critical qualities are thoroughly discussed and
    referenced throughout the plan.
  \item Relevant documents such as SRS and HA are referenced and
    connected to plan.
\end{itemize}
\textbf{SRS Verification}:
\begin{itemize}
  \item Verification process is thorough and includes key stakeholders.
  \item Provided checklist can guide review process and bring attention
    to important parts of document.
  \item Describes a plan for documenting and implementing feedback.
  \item Criteria for evaluating SRS quality is defined.
  \item  The data collected as evidence for V\&V is clear.
\end{itemize}
\textbf{Design Verification}:
\begin{itemize}
  \item Design review methods are specified, explained and justified.
  \item The process for documenting and resolving design review
    feedback is described.
  \item  The data collected as evidence for V\&V is clear.
  \item Automated testing and verification tools are specified.
\end{itemize}
\textbf{V\&V Plan Verification}:
\begin{itemize}
  \item Verification methods are specified, explained and justified.
  \item Mutation testing will be used to verify the effectiveness of unit tests.
  \item The data collected as evidence for V\&V is clear.
\end{itemize}
\textbf{System Tests for Requirements}:
\begin{itemize}
  \item All test cases are detailed and specify input data.
  \item Survey questions are outlined for usability testing.
  \item System tests connect and cover all system requirements.
  \item Traceability between test cases and requirement is clear and documented.
\end{itemize}

\subsection{Design Documents Checklist}
\label{appendix:dd_checklist}
\textbf{Module Guide}:
\begin{itemize}
  \item Each identified module follows the “one module, one secret” rule.
  \item “Uses” relation forms a clear hierarchy and represents dependency.
  \item Secrets are expressed as nouns or concepts, not actions.
  \item Traceability matrix shows that every requirement is satisfied
    by at least one module.
  \item Traceability matrix shows that every module satisfies at least
    one requirement.
  \item Traceability matrix shows that every likely change maps to a module.
  \item Behaviour-Hiding modules trace back to requirements.
  \item Software-Decision Hiding modules introduce necessary design concepts.
  \item Each Software-Decision Hiding module supports at least one
    Behaviour-Hiding module.
  \item Anticipated changes include all likely changes from SRS.
\end{itemize}

\textbf{Module Interface Specification}:
\begin{itemize}
  \item Data-only modules are modelled as exported types.
  \item Modules with state and behaviour are correctly defined as ADTs.
  \item Single-instance modules are correctly identified as Abstract Objects.
  \item Behaviour-only modules are defined as Library Modules (no state).
  \item Generic modules use the Generic keyword appropriately.
  \item Abstract Objects include proper initialization methods and assumptions.
  \item Exported constants are literal, compile-time values.
  \item Modified Hoffmann and Strooper notation (or equivalent) is used
    consistently.
  \item All local functions are used somewhere in the module specification.
  \item Each access program has a clear purpose (output or state change).
  \item State transitions clearly indicate what changes and where.
  \item State invariants hold before and after access program execution.
  \item Specification is consistent, essential, general, and
    independent of implementation details.
  \item Every module in the Module Guide (MG) appears in the MIS.
\end{itemize}

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions}
\label{appendix:usability}

\begin{enumerate}[label=UA-Q \arabic*., wide=0pt, leftmargin=*]
  \item \emph{What operating system and its version are you using?
    (i.e. Windows, MacOS, etc.)}\\[2mm]
    {\bf Purpose:} This is to collect information on the user to assess
    the compatibility across different platforms and
    whether accessibility or usability issues are system-dependent.

  \item \emph{What browser are you using? (i.e. Google Chrome, Mozilla
    Firefox, etc.)}\\[2mm]
    {\bf Purpose:} This is to collect information on the user to assess
    the compatibility across different browsers and
    whether accessibility or usability issues are dependent on the browsers.

  \item \emph{What assistive technologies or tools are you using (i.e.,
      screen readers, magnifiers, voice control), and what specific model
    or version, if applicable?}\\[2mm]
    {\bf Purpose:} This collects information on the types and versions
    of assistive technologies used by participants to understand how
    different tools interact with the web tool
    and to identify any issues specific to certain technologies.

  \item \emph{Was the length of the generated alternative text
    description too short, sufficient, or too long?}\\[2mm]
    {\bf Purpose:} This is to gain feedback on whether the length of
    the text description is sufficient enough
    for the user to gain understanding on the learning objective of the
    uploaded image.

  \item \emph{Did the generated alternative text contain too much
      irrelevant information? If so, how did this affect
    your ability to learn?}\\[2mm]
    {\bf Purpose:} This is to gain feedback on whether the alternative
    text contained irrelevant information that deviated
    from the main learning objective of the uploaded image. It is also
    to understand what difficulties the stakeholder experiences
    when there is too much irrelevant information.

  \item \emph{Was the generated alternative text contain over-detailed?
      If so, how did this affect
    your ability to learn?}\\[2mm]
    {\bf Purpose:} This is to determine whether the generated
    alternative text includes excessive detail that may hinder user
    comprehension or distract from the key information needed for
    effective learning.

  \item \emph{On a scale of 0-4, is the generated alternative text
      presented in an accessible way? (0 = not accessible at all, 4 =
    very accessible)}\\[2mm]
    {\bf Purpose:} This is to ensure that the AI-generated alternative
    text is presented in a manner that aligns with accessibility standards and
    can be easily perceived, understood, and utilized by users relying
    on assistive technologies such as screen readers.

  \item \emph{On a scale of 0–4, how easy was it for you to use and
      understand the generated alternative text? (0 = not easy at all, 4
    = very easy)}\\[2mm]
    {\bf Purpose:} This is to assess whether the generated alternative
    text is presented in a way that supports ease of use and whether it
    allows users to easily interact with
    the interface and understand the content with minimal confusion or effort.

  \item \emph{What is your primary purpose for using this web tool, and
      how do you typically use it during your tasks or learning
    activities?}\\[2mm]
    {\bf Purpose:} This is to understand the user’s main goals and
    usage patterns when interacting with the web tool and to identify
    any areas for improvement in how the web tool's features
    can better support their needs.

  \item \emph{What changes or improvements would you suggest to make
      this web tool more accessible and enhance your learning experience,
    if any?}\\[2mm]
    {\bf Purpose:} This question aims to collect additional user
    feedback on accessibility and usability improvements that may not
    have been addressed in the previous questions of this survey.
\end{enumerate}

\subsection{Glossary of All Terms}

\paragraph*{Accuracy}
The degree to which generated descriptions capture the image’s
content correctly.

\paragraph*{AI (artificial intelligence)}
Techniques that enable computers to perform tasks that normally
require human intelligence.

\paragraph*{Alt Text (Alternative Text)}
Textual description of non-text content such as images that allow
accessibility tools such as screen readers to convey the content.

\paragraph*{AODA (Accessibility for Ontarians with Disabilities Act)}
Ontario law aimed at improving accessibility for people with
disabilities by removing and preventing barriers when designing.

\paragraph*{API (Application Programming Interface)}
Rules and protocols that allows different software programs to
communicate with each other.

\paragraph*{Backend}
Server components handling processes of an application that users don't see.

\paragraph*{Benchmarking}
The comparing of performance or quality of one's system against known
systems or datasets.

\paragraph*{Contrast Ratio}
Luminance difference between text and background required by WCAG 2.1.

\paragraph*{Dataset Bias}
Systematic skew in training data that can harm fairness or accuracy
of the model.

\paragraph*{Edge Case}
Uncommon input or scenario that the system must handle safely.

\paragraph*{FIPPA (Freedom of Information and Protection of Privacy Act)}
Ontario privacy law affecting the university data in the Authentication process.

\paragraph*{Frontend}
User interface in the browser that handles input, feedback, and
accessibility features.

\paragraph*{Git/Github}
Version control and collaboration platform.

\paragraph*{HTTP/HTTPS}
Web protocols in which HTTPS adds a transport layer security
encryption for integrity and privacy.

\paragraph*{Issue (Github)}
Tracked unit of task, bug, or feature with discussion and linkage to
commits in Github.

\paragraph*{JAWS (Job Access with Speech)}
A screen reader software available on Windows.

\paragraph*{JSON (JavaScript Object Notation) / YAML (Yet Another
Markup Language)}
Human-readable data formats used for configs and API payloads.

\paragraph*{Latency}
Time from user action such as uploading an image to a system response
or alt text generation

\paragraph*{Low Vision}
Reduced level of vision that interferes with daily activities and is
to be considered in designing the user interface and testing.

\paragraph*{Manual Accessibility Testing}
Human review or testing of user interface and alt text.

\paragraph*{Modularity}
Separating user interface, vision, language, and validation for maintainability.

\paragraph*{NVDA (NonVisual Desktop Access)}
A free screen reader available on Windows.

\paragraph*{OCR (Optical Character Recognition)}
Extracts embedded text in images or diagrams.

\paragraph*{PII (Personally Identifiable Information)}
Data that identifies a person and must not appear in outputs or logs
for security.

\paragraph*{Screen Magnifier}
Assistive technology to enlarge screen content.

\paragraph*{Screen Reader}
Assistive technology that reads text aloud.

\paragraph*{Session History}
Record of user uploads and generated alt text during the current session.

\paragraph*{Stakeholder}
Anyone affected by or influencing the system.

\paragraph*{Technical Diagram}
An informational visual used in post-secondary course materials.

\paragraph*{TLS (Transport Layer Security)}
Protocol providing encryption and integrity.

\paragraph*{WCAG 2.1 (Web Content Accessibility Guidelines)}
International standard for accessible web content.

\paragraph*{WCAG Levels (A/AA/AAA)}
Different conformance tiers to WCAG 2.1 where Level AA is the target
for the project.

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\textbf{Fiza Sehar}
\begin{enumerate}
  \item What went well while writing this deliverable?

    We effectively organized the verification and validation framework
    for \textit{Reading4All} by referencing both functional and
    non-functional requirements from the SRS. We collaborated to design
    clear, structured unit tests, ensuring traceability and consistency
    across modules.

  \item What pain points did you experience during this deliverable,
    and how did you resolve them?

    This section was not required, but we completed it in detail before
    realizing that, which limited our focus on other sections. This
    caused minor conflicts regarding time and task distribution, which
    we resolved through open communication and by redistributing
    responsibilities for future deliverables. We decided to created
    internal rubrics and clearer priorities to stay organized and avoid
    similar issues moving forward.

\end{enumerate}

\textbf{Moly Mikhail Reflection}
\begin{enumerate}
  \item \textbf{What went well while writing this deliverable?}\\
    I believe many aspects went well while writing this deliverable.
    Firstly, starting our work prior to the interim TA presentation
    was really helpful.
    This provided an opportunity to look ahead at the sections and
    begin tackling them. While doing so, many questions and areas of
    confusion arose, so having our upcoming meeting with our TA was
    really helpful.
    Ultimately, this meeting allowed us to clear up any confusion
    about sections with the document, giving the group much more
    confidence. Another thing that went well throughout this
    deliverable is having team check-ins and reviews.
    This allowed all team members to gain insight into the other
    parts of the document and ensure we are aligned with the content.
  \item \textbf{What pain points did you experience during this
      deliverable, and how
    did you resolve them?}\\
    One pain point I experienced writing this deliverable was fully
    understanding the difference between verification and validation.
    This made it challenging to differentiate between the parts in section 3.
    I resolved this pain point by reviewing lecture content,
    researching more about validation and verification techniques and
    finally clarifying with our TA during the interim presentation.
    Having a good understanding about the difference between
    validation and verification was essential in completing this
    deliverable. Once I completed
    my assigned sections, I reflected back on if I had completed the
    appropriate activities, ensuring it correctly aligned with what
    the section needed.
\end{enumerate}

\textbf{Casey Francine Bulaclac - Reflection}
\begin{enumerate}
  \item What went well while writing this deliverable?\\
    What went well during this deliverable was gaining a better
    understanding of the difference between verification and validation
    and how each plays a role in making sure
    our system works as intended. It also went smoothly linking our VnV
    activities back to the requirements in the SRS, which helped us see
    clearly how each test connects to what
    the system is supposed to do. Overall, this deliverable helped our
    team prepare for how to properly test the product we will be
    developing, and also helped us gain
    insight on how verification and validation ensure that our system
    meets both its functional requirements and user needs.
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?\\
    A pain point while writing the VnV plan was trying to differentiate
    between the different
    sections of Section 3. For example, at the beginning stages, it was
    hard to understand the difference between
    Implementation Verification and Software Validation. To resolve
    this, I made sure to ask for clarification
    from our TA to ensure that I understood each section properly and
    also used sample VnV documents from teams in previous years as
    guidance when writing these sections. Another pain point
    experienced during this deliverable was a miscommunication
    regarding the division
    of work, specifically, assigning Section 5 as a task during the
    initial stage when it was not yet required. To resolve this, our team
    held a meeting to address the communication gap and established a
    goal to ensure that all members clearly understand the scope and
    requirements of each section
    before starting future deliverables.
\end{enumerate}

\end{document}
