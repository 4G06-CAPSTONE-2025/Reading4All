\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{enumitem}

\setstretch{1.15}

\title{Personal Research Plan: Alt Text Generation for Instructional Physics Diagrams}
\author{}
\date{}

\begin{document}
\maketitle
\vspace{-1.5cm}

\section*{Research Goal (February Demo)}
The goal of this personal research is to develop a trained model that generates two to four sentence alt text for instructional physics diagrams by the February demo.

Generated captions will:
\begin{itemize}
    \item describe only elements visible in the diagram
    \item explain what is happening in the diagram
    \item focus on entities and relationships such as forces and directions
    \item use factual, non-speculative language
    \item avoid unnecessary phrases such as ``image of''
\end{itemize}

This work is conducted independently on a separate development branch.

\section{Refined Problem Scope}
Rather than treating all STEM diagrams as a single task, this research narrows the problem to instructional physics diagrams used in first-year engineering courses. Broad STEM coverage would introduce large variation in diagram structure and annotation style and would require labeling and validating thousands of images across multiple disciplines, which is not realistic within the project timeline.

By focusing on first-year engineering physics, the task remains well-defined and feasible while still addressing a meaningful use case for students.

\section{Dataset Alignment and Preparation}

Some open-source diagram datasets, such as \textbf{SciCap} and \textbf{AI2D}, provide useful reference material but were created for different objectives. SciCap focuses on academic paper figure captions that often assume surrounding context, while AI2D emphasizes diagram structure rather than explanatory descriptions. Using these datasets directly would still require substantial relabeling to meet accessibility and instructional requirements.\\

For this reason, a task-aligned dataset should be used for the dataset rather than relying on open-source alternatives.

To prepare the dataset:
\begin{itemize}
    \item diagrams are grouped by physics topic and diagram type
    \item captions are normalized to a consistent instructional style
    \item speculative or contextual language is removed
\end{itemize}

This preparation step is necessary to ensure that the model learns a clear mapping between visual elements and explanatory descriptions.


\section{Dataset and Annotation Protocol}
The dataset contains approximately 1,000 instructional physics diagrams, each paired with a human-written alt-text description intended for student use.

Each caption:
\begin{itemize}
    \item is two to four sentences long
    \item mentions only visible elements
    \item describes what is happening in the diagram
    \item focuses on entities and relationships
    \item avoids speculation and unnecessary phrasing
\end{itemize}

Numeric values are described conceptually unless exact values are required for understanding.

The data is split into training, validation, and test sets using an 80 percent, 10 percent, and 10 percent split, stratified by diagram type.\\

In later stages of this research, the dataset can be expanded with additional labeled diagrams. Data augmentation techniques may also be explored to improve robustness, provided they preserve the semantic structure and instructional meaning of the diagrams. This can overall increase dataset size as well. 

\subsection{Data Labeling Format}

BLIP-2 is trained on paired image and text inputs, where each image is associated with a single target caption.
In this research, each training sample consists of an instructional physics diagram and a corresponding human-written alt-text description.\\

Labels are stored in a simple JSON-based format, with each entry containing a reference to the image file and its caption.\\

\texttt{\{ "image": "coil\_diagram\_12.png", "text": "A rectangular coil is placed in a uniform magnetic field..." \}}
\section{Modeling Approach}

The task is formulated as image-to-text generation, where an instructional physics diagram is mapped to a short natural language description. Given the dataset size, training a vision and language model from scratch is not practical, so a transfer learning approach is used.

The selected model is BLIP-2 with a FLAN-T5 language backbone. BLIP-2 uses a frozen image encoder and a Querying Transformer to extract relevant visual information from the diagram, which is then passed to a pretrained language model to generate text conditioned on the image.

This architecture is well suited for instructional diagrams because it emphasizes structured visual elements such as arrows, spatial relationships, and component interactions. The instruction-tuned FLAN-T5 backbone further supports generating concise, factual descriptions that follow predefined stylistic constraints.

Model link:
\begin{center}
\url{https://huggingface.co/Salesforce/blip2-flan-t5-xl}
\end{center}

\end{document}