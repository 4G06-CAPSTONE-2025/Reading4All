# Project Name: Reference Information

This folder holds information and resources of interest for the project.  This
is intended to be a convenient location for project members to access
support material for the project.

For LaTeX files references are put in the References.bib file for processing by
BibTeX.  A nice convention for naming the references is to use the Nat Bib
convention.  That is, the citation name, and the corresponding file name should
be in the format AuthorYear.  If there is two author's the name should be
Author1Author2Year.  For more than two authors, the name should be
Author1EtAlYear.  In the preceding text, Author, Author1 and Author2 are the
last names of the authors.


### References for Model Training
* In text citations can be found in inference/README.md

[1] “Nlpconnect/VIT-GPT2-image-captioning · hugging face,” nlpconnect/vit-gpt2-image-captioning · Hugging Face, https://huggingface.co/nlpconnect/vit-gpt2-image-captioning (accessed Nov. 25, 2025). 

[2] D. Shah, “Vision Transformer: What it is & how it works [2024 guide],” V7, https://www.v7labs.com/blog/vision-transformer-guide (accessed Nov. 25, 2025). 

[3] R. Lamsal, “How llms work: A beginner’s guide to decoder-only Transformers,” Langformers Blog, https://blog.langformers.com/how-llms-work/ (accessed Nov. 25, 2025). 

[4] Comment et al., “Transformers in machine learning,” GeeksforGeeks, https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/ (accessed Nov. 25, 2025). 

[5] “Salesforce/Blip-image-captioning-base · hugging face,” Salesforce/blip-image-captioning-base · Hugging Face, https://huggingface.co/Salesforce/blip-image-captioning-base (accessed Nov. 25, 2025). 

[6] A. Sabir, Paper summary: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation | by Ahmed Sabir | Medium, https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166 (accessed Nov. 26, 2025). 